{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q numpy scikit-learn pandas xgboost lightgbm category_encoders matplotlib seaborn cloudpickle shap optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced ML Pipeline - Modular Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from tabularaml.generate.features import FeatureGenerator\n",
    "from tabularaml.eval.scorers import Scorer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Generator, Tuple, Optional, Union\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.validation import check_X_y, indexable\n",
    "from sklearn.utils import check_random_state\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "\n",
    "\n",
    "class SpatialTemporalKFold(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Spatial-Temporal Cross-Validator for geographic time-series data.\n",
    "    \n",
    "    This cross-validator creates folds that respect both spatial and temporal \n",
    "    dependencies in the data, preventing data leakage in spatial-temporal \n",
    "    prediction tasks like air pollution forecasting.\n",
    "    \n",
    "    The strategy:\n",
    "    1. Creates spatial clusters using geographic coordinates (lat/lon)\n",
    "    2. Creates temporal clusters using cyclical time features\n",
    "    3. Combines spatial-temporal groups to ensure validation sets are \n",
    "       spatially and temporally separated from training sets\n",
    "    4. Optionally stratifies by target variable ranges\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    spatial_clusters : int, default=20\n",
    "        Number of spatial clusters for geographic grouping\n",
    "    temporal_clusters : int, default=8\n",
    "        Number of temporal clusters for time-based grouping\n",
    "    lat_col : str, default='latitude'\n",
    "        Column name for latitude coordinates\n",
    "    lon_col : str, default='longitude' \n",
    "        Column name for longitude coordinates\n",
    "    time_cols : dict, default=None\n",
    "        Dictionary mapping time column names to their cycles:\n",
    "        {'day_of_year': 365, 'hour': 24, 'day_of_week': 7, 'month': 12}\n",
    "    stratify : bool, default=True\n",
    "        Whether to stratify splits by target variable quantiles\n",
    "    n_quantiles : int, default=5\n",
    "        Number of quantiles for stratification (if stratify=True)\n",
    "    buffer_distance : float, default=0.1\n",
    "        Minimum spatial distance between train/validation clusters (degrees)\n",
    "    temporal_buffer : int, default=7\n",
    "        Minimum temporal distance between train/validation (in days)\n",
    "    random_state : int, default=None\n",
    "        Random state for reproducible splits\n",
    "    shuffle : bool, default=True\n",
    "        Whether to shuffle data before splitting\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from spatial_temporal_cv import SpatialTemporalKFold\n",
    "    >>> \n",
    "    >>> # Basic usage\n",
    "    >>> cv = SpatialTemporalKFold(n_splits=5, random_state=42)\n",
    "    >>> X = df[['latitude', 'longitude', 'day_of_year', 'hour', 'month']]\n",
    "    >>> y = df['pollution_value']\n",
    "    >>> \n",
    "    >>> for train_idx, val_idx in cv.split(X, y):\n",
    "    >>>     X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    >>>     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    >>>     # Train your model...\n",
    "    >>>\n",
    "    >>> # Custom time columns\n",
    "    >>> time_cols = {'day_of_year': 365, 'hour': 24, 'day_of_week': 7}\n",
    "    >>> cv = SpatialTemporalKFold(\n",
    "    >>>     n_splits=3, \n",
    "    >>>     time_cols=time_cols,\n",
    "    >>>     buffer_distance=0.05,\n",
    "    >>>     temporal_buffer=14\n",
    "    >>> )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_splits: int = 5,\n",
    "                 spatial_clusters: int = 20,\n",
    "                 temporal_clusters: int = 8,\n",
    "                 lat_col: str = 'latitude',\n",
    "                 lon_col: str = 'longitude',\n",
    "                 time_cols: Optional[dict] = None,\n",
    "                 stratify: bool = True,\n",
    "                 n_quantiles: int = 5,\n",
    "                 buffer_distance: float = 0.1,\n",
    "                 temporal_buffer: int = 7,\n",
    "                 random_state: Optional[int] = None,\n",
    "                 shuffle: bool = True):\n",
    "        \n",
    "        self.n_splits = n_splits\n",
    "        self.spatial_clusters = spatial_clusters\n",
    "        self.temporal_clusters = temporal_clusters\n",
    "        self.lat_col = lat_col\n",
    "        self.lon_col = lon_col\n",
    "        self.time_cols = time_cols or {\n",
    "            'day_of_year': 365, 'hour': 24, 'day_of_week': 7, 'month': 12\n",
    "        }\n",
    "        self.stratify = stratify\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.buffer_distance = buffer_distance\n",
    "        self.temporal_buffer = temporal_buffer\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Validation\n",
    "        if n_splits < 2:\n",
    "            raise ValueError(\"n_splits must be at least 2\")\n",
    "        if spatial_clusters < n_splits:\n",
    "            warnings.warn(f\"spatial_clusters ({spatial_clusters}) < n_splits ({n_splits}). \"\n",
    "                         \"This may result in poor spatial separation.\")\n",
    "        if temporal_clusters < 2:\n",
    "            raise ValueError(\"temporal_clusters must be at least 2\")\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Returns the number of splitting iterations.\"\"\"\n",
    "        return self.n_splits\n",
    "    \n",
    "    def _validate_data(self, X: Union[pd.DataFrame, np.ndarray], \n",
    "                      y: Optional[Union[pd.Series, np.ndarray]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Validate and convert input data to DataFrame.\"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            if isinstance(X, np.ndarray):\n",
    "                # Try to infer column names for numpy arrays\n",
    "                expected_cols = [self.lat_col, self.lon_col] + list(self.time_cols.keys())\n",
    "                if X.shape[1] >= len(expected_cols):\n",
    "                    X = pd.DataFrame(X, columns=expected_cols[:X.shape[1]])\n",
    "                else:\n",
    "                    raise ValueError(f\"Expected at least {len(expected_cols)} columns, got {X.shape[1]}\")\n",
    "            else:\n",
    "                raise TypeError(\"X must be pandas DataFrame or numpy array\")\n",
    "        \n",
    "        # Check required columns exist\n",
    "        missing_cols = []\n",
    "        if self.lat_col not in X.columns:\n",
    "            missing_cols.append(self.lat_col)\n",
    "        if self.lon_col not in X.columns:\n",
    "            missing_cols.append(self.lon_col)\n",
    "        for col in self.time_cols.keys():\n",
    "            if col not in X.columns:\n",
    "                missing_cols.append(col)\n",
    "                \n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def _create_spatial_clusters(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create spatial clusters using K-means on lat/lon coordinates.\"\"\"\n",
    "        coords = X[[self.lat_col, self.lon_col]]\n",
    "        \n",
    "        # Handle edge case where we have fewer samples than clusters\n",
    "        n_clusters = min(self.spatial_clusters, len(coords))\n",
    "        if n_clusters < self.spatial_clusters:\n",
    "            warnings.warn(f\"Reducing spatial_clusters from {self.spatial_clusters} to {n_clusters} \"\n",
    "                         f\"due to insufficient data points.\")\n",
    "        \n",
    "        kmeans = KMeans(\n",
    "            n_clusters=n_clusters, \n",
    "            random_state=self.random_state,\n",
    "            n_init=10\n",
    "        )\n",
    "        spatial_labels = kmeans.fit_predict(coords.copy().fillna(coords.mean()).values)\n",
    "        self._spatial_centroids = kmeans.cluster_centers_\n",
    "        \n",
    "        return spatial_labels\n",
    "    \n",
    "    def _create_temporal_features(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Convert cyclical time features to circular coordinates.\"\"\"\n",
    "        temporal_features = []\n",
    "        \n",
    "        for col, cycle_length in self.time_cols.items():\n",
    "            if col in X.columns:\n",
    "                # Convert to radians for circular representation\n",
    "                radians = 2 * np.pi * X[col] / cycle_length\n",
    "                # Use sine and cosine to capture cyclical nature\n",
    "                temporal_features.extend([np.sin(radians), np.cos(radians)])\n",
    "        \n",
    "        return np.column_stack(temporal_features) if temporal_features else np.zeros((len(X), 2))\n",
    "    \n",
    "    def _create_temporal_clusters(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create temporal clusters using cyclical time features.\"\"\"\n",
    "        temporal_coords = self._create_temporal_features(X)\n",
    "        \n",
    "        # Handle edge case where we have fewer samples than clusters\n",
    "        n_clusters = min(self.temporal_clusters, len(temporal_coords))\n",
    "        if n_clusters < self.temporal_clusters:\n",
    "            warnings.warn(f\"Reducing temporal_clusters from {self.temporal_clusters} to {n_clusters} \"\n",
    "                         f\"due to insufficient data points.\")\n",
    "        \n",
    "        kmeans = KMeans(\n",
    "            n_clusters=n_clusters,\n",
    "            random_state=self.random_state,\n",
    "            n_init=10\n",
    "        )\n",
    "        temporal_labels = kmeans.fit_predict(temporal_coords)\n",
    "        self._temporal_centroids = kmeans.cluster_centers_\n",
    "        \n",
    "        return temporal_labels\n",
    "    \n",
    "    def _create_stratification_groups(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create stratification groups based on target variable quantiles.\"\"\"\n",
    "        if not self.stratify or y is None:\n",
    "            return np.zeros(len(y), dtype=int)\n",
    "        \n",
    "        # Create quantile-based groups\n",
    "        quantiles = np.linspace(0, 1, self.n_quantiles + 1)\n",
    "        quantile_values = np.quantile(y, quantiles)\n",
    "        \n",
    "        # Assign each sample to a quantile group\n",
    "        strat_groups = np.digitize(y, quantile_values[1:-1])\n",
    "        \n",
    "        return strat_groups\n",
    "    \n",
    "    def _check_spatial_separation(self, train_spatial_groups: np.ndarray, \n",
    "                                 val_spatial_groups: np.ndarray) -> bool:\n",
    "        \"\"\"Check if spatial clusters are sufficiently separated.\"\"\"\n",
    "        if not hasattr(self, '_spatial_centroids'):\n",
    "            return True\n",
    "            \n",
    "        train_centroids = self._spatial_centroids[train_spatial_groups]\n",
    "        val_centroids = self._spatial_centroids[val_spatial_groups]\n",
    "        \n",
    "        # Calculate minimum distance between train and validation centroids\n",
    "        distances = cdist(train_centroids, val_centroids)\n",
    "        min_distance = np.min(distances)\n",
    "        \n",
    "        return min_distance >= self.buffer_distance\n",
    "    \n",
    "    def _create_combined_groups(self, X: pd.DataFrame, y: Optional[np.ndarray] = None) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"Create combined spatial-temporal-stratification groups.\"\"\"\n",
    "        # Create individual groupings\n",
    "        spatial_labels = self._create_spatial_clusters(X)\n",
    "        temporal_labels = self._create_temporal_clusters(X)\n",
    "        \n",
    "        if y is not None:\n",
    "            strat_labels = self._create_stratification_groups(y)\n",
    "        else:\n",
    "            strat_labels = np.zeros(len(X), dtype=int)\n",
    "        \n",
    "        # Combine all groupings into unique identifiers\n",
    "        max_spatial = np.max(spatial_labels) + 1\n",
    "        max_temporal = np.max(temporal_labels) + 1\n",
    "        max_strat = np.max(strat_labels) + 1\n",
    "        \n",
    "        combined_groups = (spatial_labels * max_temporal * max_strat + \n",
    "                          temporal_labels * max_strat + \n",
    "                          strat_labels)\n",
    "        \n",
    "        # Store metadata for analysis\n",
    "        metadata = {\n",
    "            'spatial_labels': spatial_labels,\n",
    "            'temporal_labels': temporal_labels,\n",
    "            'strat_labels': strat_labels,\n",
    "            'n_spatial_clusters': max_spatial,\n",
    "            'n_temporal_clusters': max_temporal,\n",
    "            'n_strat_groups': max_strat,\n",
    "            'n_combined_groups': len(np.unique(combined_groups))\n",
    "        }\n",
    "        \n",
    "        return combined_groups, metadata\n",
    "    \n",
    "    def split(self, X: Union[pd.DataFrame, np.ndarray], \n",
    "              y: Optional[Union[pd.Series, np.ndarray]] = None, \n",
    "              groups: Optional[np.ndarray] = None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "        \"\"\"\n",
    "        Generate indices to split data into training and test set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame or array-like of shape (n_samples, n_features)\n",
    "            Training data with spatial and temporal features\n",
    "        y : array-like of shape (n_samples,), optional\n",
    "            Target variable for stratification\n",
    "        groups : array-like of shape (n_samples,), optional\n",
    "            Not used, present for API compatibility\n",
    "            \n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split\n",
    "        test : ndarray  \n",
    "            The testing set indices for that split\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X, y)\n",
    "        X, y = indexable(X, y)\n",
    "        \n",
    "        if y is not None:\n",
    "            y = np.asarray(y)\n",
    "            \n",
    "        rng = check_random_state(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            rng.shuffle(indices)\n",
    "            X = X.iloc[indices].reset_index(drop=True)\n",
    "            if y is not None:\n",
    "                y = y[indices]\n",
    "        \n",
    "        # Create spatial-temporal groups\n",
    "        combined_groups, metadata = self._create_combined_groups(X, y)\n",
    "        unique_groups = np.unique(combined_groups)\n",
    "        \n",
    "        if len(unique_groups) < self.n_splits:\n",
    "            raise ValueError(f\"Cannot create {self.n_splits} splits with only \"\n",
    "                           f\"{len(unique_groups)} unique spatial-temporal groups. \"\n",
    "                           f\"Consider reducing n_splits or clustering parameters.\")\n",
    "        \n",
    "        # Shuffle groups for random assignment to folds\n",
    "        rng.shuffle(unique_groups)\n",
    "        \n",
    "        # Assign groups to folds using round-robin to balance sizes\n",
    "        fold_groups = [[] for _ in range(self.n_splits)]\n",
    "        for i, group in enumerate(unique_groups):\n",
    "            fold_groups[i % self.n_splits].append(group)\n",
    "        \n",
    "        # Generate train/test splits\n",
    "        for fold_idx in range(self.n_splits):\n",
    "            test_groups = np.array(fold_groups[fold_idx])\n",
    "            # Create train groups deterministically by excluding test groups\n",
    "            train_groups = unique_groups[~np.isin(unique_groups, test_groups)]\n",
    "            \n",
    "            test_mask = np.isin(combined_groups, test_groups)\n",
    "            train_mask = np.isin(combined_groups, train_groups)\n",
    "            \n",
    "            test_indices = indices[test_mask] if self.shuffle else np.where(test_mask)[0]\n",
    "            train_indices = indices[train_mask] if self.shuffle else np.where(train_mask)[0]\n",
    "            \n",
    "            # Validate split quality\n",
    "            if len(test_indices) == 0:\n",
    "                warnings.warn(f\"Fold {fold_idx} has empty test set\")\n",
    "                continue\n",
    "            if len(train_indices) == 0:\n",
    "                warnings.warn(f\"Fold {fold_idx} has empty train set\")\n",
    "                continue\n",
    "                \n",
    "            yield train_indices, test_indices\n",
    "    \n",
    "    def get_split_info(self, X: Union[pd.DataFrame, np.ndarray], \n",
    "                      y: Optional[Union[pd.Series, np.ndarray]] = None) -> dict:\n",
    "        \"\"\"\n",
    "        Get detailed information about the splits without generating them.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing split statistics and metadata\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X, y)\n",
    "        if y is not None:\n",
    "            y = np.asarray(y)\n",
    "            \n",
    "        combined_groups, metadata = self._create_combined_groups(X, y)\n",
    "        \n",
    "        info = {\n",
    "            'n_samples': len(X),\n",
    "            'n_splits': self.n_splits,\n",
    "            **metadata,\n",
    "            'avg_samples_per_group': len(X) / metadata['n_combined_groups'],\n",
    "            'spatial_clusters_used': metadata['n_spatial_clusters'],\n",
    "            'temporal_clusters_used': metadata['n_temporal_clusters']\n",
    "        }\n",
    "        \n",
    "        # Calculate split size statistics\n",
    "        unique_groups = np.unique(combined_groups)\n",
    "        fold_groups = [[] for _ in range(self.n_splits)]\n",
    "        for i, group in enumerate(unique_groups):\n",
    "            fold_groups[i % self.n_splits].append(group)\n",
    "            \n",
    "        fold_sizes = []\n",
    "        for fold_idx in range(self.n_splits):\n",
    "            test_groups = np.array(fold_groups[fold_idx])\n",
    "            test_mask = np.isin(combined_groups, test_groups)\n",
    "            fold_sizes.append(np.sum(test_mask))\n",
    "            \n",
    "        info.update({\n",
    "            'fold_sizes': fold_sizes,\n",
    "            'min_fold_size': min(fold_sizes),\n",
    "            'max_fold_size': max(fold_sizes),\n",
    "            'fold_size_std': np.std(fold_sizes)\n",
    "        })\n",
    "        \n",
    "        return info\n",
    "\n",
    "\n",
    "class StratifiedSpatialTemporalKFold(SpatialTemporalKFold):\n",
    "    \"\"\"\n",
    "    Stratified version that ensures better target distribution balance.\n",
    "    \n",
    "    This extends SpatialTemporalKFold with enhanced stratification that\n",
    "    tries to maintain similar target distributions across folds while\n",
    "    still respecting spatial-temporal constraints.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        # Force stratification on\n",
    "        kwargs['stratify'] = True\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Split with enhanced stratification checking.\"\"\"\n",
    "        if y is None:\n",
    "            raise ValueError(\"StratifiedSpatialTemporalKFold requires y for stratification\")\n",
    "            \n",
    "        # Generate base splits\n",
    "        for train_idx, test_idx in super().split(X, y, groups):\n",
    "            # Check stratification quality\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Calculate quantile distributions\n",
    "            train_quantiles = np.quantile(y_train, [0.25, 0.5, 0.75])\n",
    "            test_quantiles = np.quantile(y_test, [0.25, 0.5, 0.75]) \n",
    "            \n",
    "            # Check if distributions are too different\n",
    "            max_diff = np.max(np.abs(train_quantiles - test_quantiles))\n",
    "            if max_diff > 2 * np.std(y):\n",
    "                warnings.warn(f\"Large distribution difference detected: {max_diff:.4f}\")\n",
    "                \n",
    "            yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import BaseCrossValidator\n",
    "import numpy as np\n",
    "\n",
    "class FixedWindowTimeSeriesSplit(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Custom time-series cross-validator with fixed-size test windows.\n",
    "    Ensures every fold has meaningful training data and proper temporal ordering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int\n",
    "        Number of folds. Must be at least 1.\n",
    "    test_size : int\n",
    "        Number of samples in each test fold.\n",
    "    gap : int, default=0\n",
    "        Number of samples to exclude between train and test sets.\n",
    "    min_train_size : int, default=None\n",
    "        Minimum number of training samples required. If None, defaults to test_size.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, test_size=2700, gap=0, min_train_size=None):\n",
    "        if n_splits < 1:\n",
    "            raise ValueError(\"n_splits must be at least 1.\")\n",
    "        if test_size < 1:\n",
    "            raise ValueError(\"test_size must be at least 1.\")\n",
    "        if gap < 0:\n",
    "            raise ValueError(\"gap must be non-negative.\")\n",
    "        \n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.gap = gap\n",
    "        self.min_train_size = min_train_size or test_size\n",
    "        \n",
    "        if self.min_train_size < 1:\n",
    "            raise ValueError(\"min_train_size must be at least 1.\")\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Check if we have enough data for at least one split\n",
    "        min_required = self.min_train_size + self.gap + self.test_size\n",
    "        if min_required > n_samples:\n",
    "            raise ValueError(\n",
    "                f\"Not enough samples. Need at least {min_required} samples \"\n",
    "                f\"(min_train_size={self.min_train_size} + gap={self.gap} + test_size={self.test_size}), \"\n",
    "                f\"but got {n_samples}.\"\n",
    "            )\n",
    "        \n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        if self.n_splits == 1:\n",
    "            # Single window: place test at the end, ensure minimum training size\n",
    "            test_end = n_samples\n",
    "            test_start = test_end - self.test_size\n",
    "            train_end = test_start - self.gap\n",
    "            \n",
    "            # Ensure we have minimum training size\n",
    "            if train_end < self.min_train_size:\n",
    "                train_end = self.min_train_size\n",
    "                test_start = train_end + self.gap\n",
    "                test_end = test_start + self.test_size\n",
    "                \n",
    "                # Check if this fits within our data\n",
    "                if test_end > n_samples:\n",
    "                    raise ValueError(\n",
    "                        f\"Cannot fit single split with constraints. \"\n",
    "                        f\"Need {self.min_train_size + self.gap + self.test_size} samples, got {n_samples}.\"\n",
    "                    )\n",
    "            \n",
    "            train_idx = indices[:train_end]\n",
    "            test_idx = indices[test_start:test_end]\n",
    "            yield train_idx, test_idx\n",
    "            return\n",
    "        \n",
    "        # For multiple splits, distribute test windows\n",
    "        # Last test window ends at n_samples, work backwards\n",
    "        test_windows = []\n",
    "        \n",
    "        # Calculate positions for test windows\n",
    "        # We want to distribute them evenly in the available space\n",
    "        latest_test_end = n_samples\n",
    "        earliest_test_start = self.min_train_size + self.gap\n",
    "        \n",
    "        # Available space for test window starts\n",
    "        available_space = latest_test_end - self.test_size - earliest_test_start\n",
    "        \n",
    "        if available_space < 0:\n",
    "            raise ValueError(\n",
    "                \"Cannot create requested splits. Try reducing n_splits, test_size, or min_train_size.\"\n",
    "            )\n",
    "        \n",
    "        # Calculate step size between test windows\n",
    "        if self.n_splits == 1:\n",
    "            step = 0\n",
    "        else:\n",
    "            step = available_space / (self.n_splits - 1)\n",
    "        \n",
    "        # Generate test windows from last to first\n",
    "        for i in range(self.n_splits):\n",
    "            # Calculate test window position\n",
    "            test_start = int(earliest_test_start + i * step)\n",
    "            test_end = test_start + self.test_size\n",
    "            \n",
    "            # Ensure test window doesn't exceed data bounds\n",
    "            if test_end > n_samples:\n",
    "                test_end = n_samples\n",
    "                test_start = test_end - self.test_size\n",
    "            \n",
    "            # Calculate training end (before gap)\n",
    "            train_end = test_start - self.gap\n",
    "            \n",
    "            # Ensure minimum training size\n",
    "            if train_end < self.min_train_size:\n",
    "                raise ValueError(\n",
    "                    f\"Split {i+1} would have insufficient training data. \"\n",
    "                    f\"Try reducing n_splits or min_train_size.\"\n",
    "                )\n",
    "            \n",
    "            train_idx = indices[:train_end]\n",
    "            test_idx = indices[test_start:test_end]\n",
    "            \n",
    "            yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_exp(y_true, y_pred):\n",
    "    return np.exp(-np.sqrt(mean_squared_error(y_true, y_pred))/100)\n",
    "\n",
    "def competition_score(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return np.exp(-rmse / 100)\n",
    "\n",
    "def create_cyclical_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if 'hour' in df_copy.columns:\n",
    "        df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['hour'] / 24.0)\n",
    "        df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['hour'] / 24.0)\n",
    "    \n",
    "    if 'day_of_week' in df_copy.columns:\n",
    "        df_copy['dow_sin'] = np.sin(2 * np.pi * df_copy['day_of_week'] / 7.0)\n",
    "        df_copy['dow_cos'] = np.cos(2 * np.pi * df_copy['day_of_week'] / 7.0)\n",
    "    \n",
    "    if 'day_of_year' in df_copy.columns:\n",
    "        df_copy['doy_sin'] = np.sin(2 * np.pi * df_copy['day_of_year'] / 365.0)\n",
    "        df_copy['doy_cos'] = np.cos(2 * np.pi * df_copy['day_of_year'] / 365.0)\n",
    "    \n",
    "    columns_to_drop = ['hour', 'day_of_week', 'day_of_year', 'month']\n",
    "    existing_columns_to_drop = [col for col in columns_to_drop if col in df_copy.columns]\n",
    "    if existing_columns_to_drop:\n",
    "        df_copy = df_copy.drop(columns=existing_columns_to_drop)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_feature_generation(X_train, y_train, save_dir=\"./features\", run_id=1, max_new_feats=2000, n_generations=2000):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    splitter = SpatialTemporalKFold(n_splits=5, random_state=42)\n",
    "    rmse_exp_scorer = Scorer(name=\"rmse_exp\", scorer=rmse_exp, greater_is_better=True, extra_params={}, from_probs=False)\n",
    "    \n",
    "    print(f\"Running feature generation run {run_id}...\")\n",
    "    \n",
    "    generator = FeatureGenerator(\n",
    "        task=\"regression\",\n",
    "        scorer=rmse_exp_scorer,\n",
    "        max_new_feats=max_new_feats,\n",
    "        cv=splitter,\n",
    "        n_generations=n_generations,\n",
    "        save_path=f\"{save_dir}/feature_generator_run_{run_id}.pkl\",\n",
    "    )\n",
    "    \n",
    "    results = generator.search(X_train, y_train)\n",
    "    X_generated = generator.transform(X_train)\n",
    "    \n",
    "    print(f\"Run {run_id} complete: {X_generated.shape[1]} features generated\")\n",
    "    return X_generated, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_feature_generators(feature_dir):\n",
    "    generator_files = glob.glob(os.path.join(feature_dir, \"*.pkl\"))\n",
    "    if not generator_files:\n",
    "        raise ValueError(f\"No feature generator files found in {feature_dir}\")\n",
    "    \n",
    "    generators = []\n",
    "    print(f\"Loading {len(generator_files)} feature generators from {feature_dir}: {sorted(generator_files)}.\")\n",
    "    for file_path in sorted(generator_files):\n",
    "        generator = FeatureGenerator.load(file_path)\n",
    "        generators.append(generator)\n",
    "\n",
    "    return generators\n",
    "\n",
    "def combine_feature_generators(generators, X_train, X_test=None):\n",
    "    print(f\"Combining features from {len(generators)} generators...\")\n",
    "    X_train = X_train.copy()\n",
    "    if X_test is not None:\n",
    "        X_test = X_test.copy()\n",
    "    \n",
    "    for i, generator in enumerate(generators):\n",
    "        X_train = generator.fit_transform(X_train)\n",
    "        if X_test is not None:\n",
    "            X_test = generator.transform(X_test)\n",
    "    \n",
    "    if X_test is not None:\n",
    "        return X_train, X_test\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_objective(trial, X_base, y_train, tss, n_clusters_range=(20, 50)):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 30),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.3, 1.0),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.3, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 100.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 100.0, log=True),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 0, 2000),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'max_bin': trial.suggest_int('max_bin', 32, 512),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'enable_categorical': True,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    n_clusters = trial.suggest_int('n_clusters', n_clusters_range[0], n_clusters_range[1])\n",
    "    \n",
    "    X_base_cyclic = create_cyclical_features(X_base.copy())\n",
    "    y = np.log1p(y_train.copy())\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(tss.split(X_base_cyclic, y)):\n",
    "        X_train_fold = X_base_cyclic.iloc[train_idx].copy()\n",
    "        X_valid_fold = X_base_cyclic.iloc[valid_idx].copy()\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_valid_fold = y.iloc[valid_idx]\n",
    "        \n",
    "        if 'latitude' in X_train_fold.columns and 'longitude' in X_train_fold.columns:\n",
    "            lat_mean = X_train_fold['latitude'].mean()\n",
    "            lon_mean = X_train_fold['longitude'].mean()\n",
    "            \n",
    "            train_coords_temp = X_train_fold[['latitude', 'longitude']].copy()\n",
    "            valid_coords_temp = X_valid_fold[['latitude', 'longitude']].copy()\n",
    "            \n",
    "            train_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "            train_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "            valid_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "            valid_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "            kmeans.fit(train_coords_temp)\n",
    "            \n",
    "            X_train_fold['cluster'] = kmeans.predict(train_coords_temp)\n",
    "            X_valid_fold['cluster'] = kmeans.predict(valid_coords_temp)\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_valid_fold)\n",
    "        y_pred_orig_scale = np.expm1(y_pred_fold)\n",
    "        y_valid_orig_scale = np.expm1(y_valid_fold)\n",
    "        \n",
    "        exp_score = competition_score(y_valid_orig_scale, y_pred_orig_scale)\n",
    "        fold_scores.append(exp_score)\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(X_train, y_train, params, model_type='xgb'):\n",
    "    X_train_cyclic = create_cyclical_features(X_train.copy())\n",
    "    \n",
    "    n_clusters = params.get('n_clusters', 30)\n",
    "    if 'latitude' in X_train_cyclic.columns and 'longitude' in X_train_cyclic.columns:\n",
    "        lat_mean = X_train_cyclic['latitude'].mean()\n",
    "        lon_mean = X_train_cyclic['longitude'].mean()\n",
    "        \n",
    "        coords_temp = X_train_cyclic[['latitude', 'longitude']].copy()\n",
    "        coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "        coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "        kmeans.fit(coords_temp)\n",
    "        X_train_cyclic['cluster'] = kmeans.predict(coords_temp)\n",
    "    \n",
    "    y_train_log = np.log1p(y_train)\n",
    "    \n",
    "    if model_type == 'xgb':\n",
    "        model_params = {k: v for k, v in params.items() if k != 'n_clusters'}\n",
    "        model = xgb.XGBRegressor(**model_params)\n",
    "        model.fit(X_train_cyclic, y_train_log)\n",
    "    elif model_type == 'lgb':\n",
    "        lgb_params = {\n",
    "            'n_estimators': params.get('n_estimators', 1000),\n",
    "            'learning_rate': params.get('learning_rate', 0.05),\n",
    "            'num_leaves': 2 ** params.get('max_depth', 6) - 1,\n",
    "            'feature_fraction': params.get('colsample_bytree', 0.8),\n",
    "            'bagging_fraction': params.get('subsample', 0.8),\n",
    "            'bagging_freq': 1,\n",
    "            'lambda_l1': params.get('reg_alpha', 0),\n",
    "            'lambda_l2': params.get('reg_lambda', 1),\n",
    "            'min_data_in_leaf': int(params.get('min_child_weight', 1)),\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**lgb_params)\n",
    "        model.fit(X_train_cyclic, y_train_log, verbose=-1)\n",
    "    \n",
    "    model.kmeans_ = kmeans if 'kmeans' in locals() else None\n",
    "    model.lat_mean_ = lat_mean if 'lat_mean' in locals() else None\n",
    "    model.lon_mean_ = lon_mean if 'lon_mean' in locals() else None\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_model_ensemble(X_train, y_train, X_test, best_params, tss):\n    predictions = []\n    \n    # Apply cyclical features and clustering to test data\n    X_test_processed = create_cyclical_features(X_test.copy())\n    \n    # Add clustering to test data using the same parameters as training\n    n_clusters = best_params.get('n_clusters', 30)\n    if 'latitude' in X_test_processed.columns and 'longitude' in X_test_processed.columns:\n        # We need to get the lat/lon means from training data for consistency\n        X_train_temp = create_cyclical_features(X_train.copy())\n        lat_mean = X_train_temp['latitude'].mean()\n        lon_mean = X_train_temp['longitude'].mean()\n        \n        # Prepare test coordinates\n        test_coords_temp = X_test_processed[['latitude', 'longitude']].copy()\n        test_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n        test_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n        \n        # Prepare training coordinates for fitting KMeans\n        train_coords_temp = X_train_temp[['latitude', 'longitude']].copy()\n        train_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n        train_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n        \n        # Fit KMeans on training data and predict on test data\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n        kmeans.fit(train_coords_temp)\n        X_test_processed['cluster'] = kmeans.predict(test_coords_temp)\n    \n    print(\"Training XGBoost with best parameters...\")\n    xgb_model = train_final_model(X_train, y_train, best_params, model_type='xgb')\n    pred_xgb = xgb_model.predict(X_test_processed)\n    predictions.append(('xgb_best', pred_xgb, 0.7))\n    \n    print(\"Training LightGBM...\")\n    lgb_model = train_final_model(X_train, y_train, best_params, model_type='lgb')\n    pred_lgb = lgb_model.predict(X_test_processed)\n    predictions.append(('lgb', pred_lgb, 0.2))\n    \n    print(\"Training XGBoost variant...\")\n    xgb_variant_params = best_params.copy()\n    xgb_variant_params['max_depth'] = min(best_params['max_depth'] + 2, 15)\n    xgb_variant_params['learning_rate'] = best_params['learning_rate'] * 0.8\n    xgb_variant_model = train_final_model(X_train, y_train, xgb_variant_params, model_type='xgb')\n    pred_xgb_variant = xgb_variant_model.predict(X_test_processed)\n    predictions.append(('xgb_variant', pred_xgb_variant, 0.1))\n    \n    final_pred = np.zeros(len(X_test_processed))\n    for name, pred, weight in predictions:\n        final_pred += weight * pred\n    \n    return final_pred, predictions"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_post_processing(train_df, y_train, test_df, predictions):\n",
    "    train_quantiles = np.percentile(y_train, [10, 25, 50, 75, 90])\n",
    "    pred_quantiles = np.percentile(predictions, [10, 25, 50, 75, 90])\n",
    "    \n",
    "    if abs(train_quantiles[2] - pred_quantiles[2]) > 0.1 * train_quantiles[2]:\n",
    "        scale = train_quantiles[2] / pred_quantiles[2]\n",
    "        predictions_adjusted = predictions * scale\n",
    "    else:\n",
    "        predictions_adjusted = predictions\n",
    "    \n",
    "    test_lat_min, test_lat_max = test_df['latitude'].min(), test_df['latitude'].max()\n",
    "    train_lat_min, train_lat_max = train_df['latitude'].min(), train_df['latitude'].max()\n",
    "    \n",
    "    if test_lat_min < train_lat_min or test_lat_max > train_lat_max:\n",
    "        extreme_mask = ((test_df['latitude'] < train_lat_min) | (test_df['latitude'] > train_lat_max))\n",
    "        if extreme_mask.any():\n",
    "            mean_pred = predictions_adjusted[~extreme_mask].mean()\n",
    "            predictions_adjusted[extreme_mask] = (0.7 * predictions_adjusted[extreme_mask] + 0.3 * mean_pred)\n",
    "    \n",
    "    return predictions_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_pipeline(train_df, test_df, target_col='pollution_value', n_trials=1000, feature_dir=None):\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENHANCED PIPELINE - BUILDING ON SUCCESS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X_train = train_df.drop(target_col, axis=1)\n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df.copy()\n",
    "    \n",
    "    test_ids = test_df['id'].values\n",
    "    \n",
    "    if feature_dir and os.path.exists(feature_dir):\n",
    "        print(f\"\\nLoading existing feature generators from {feature_dir}...\")\n",
    "        generators = load_all_feature_generators(feature_dir)\n",
    "        X_train_generated, X_test_generated = combine_feature_generators(generators, X_train, X_test)\n",
    "    else:\n",
    "        print(\"\\nNo existing feature generators found. Please run Stage 1 first.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"Total features after generation: {X_train_generated.shape[1]}\")\n",
    "    \n",
    "    print(\"\\nOptimizing hyperparameters...\")\n",
    "    \n",
    "    tss = FixedWindowTimeSeriesSplit(n_splits=5, test_size=2700, gap=0, min_train_size=2700)\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(multivariate=True, group=True, n_startup_trials=20, constant_liar=True, seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=\"xgboost_optimization_enhanced\", sampler=sampler, storage=\"sqlite:///xgb_optuna_enhanced.db\", load_if_exists=True)\n",
    "    \n",
    "    objective_func = lambda trial: enhanced_objective(trial, X_train_generated, y_train, tss, n_clusters_range=(20, 50))\n",
    "    study.optimize(objective_func, n_trials=n_trials)\n",
    "    \n",
    "    best_params = study.best_params.copy()\n",
    "    print(f\"\\nBest score: {study.best_value:.4f}\")\n",
    "    \n",
    "    print(\"\\nGenerating final predictions...\")\n",
    "    final_predictions, model_predictions = create_model_ensemble(X_train_generated, y_train, X_test_generated, best_params, tss)\n",
    "    \n",
    "    print(\"\\nApplying post-processing...\")\n",
    "    final_predictions = np.maximum(final_predictions, 0)\n",
    "    final_predictions = advanced_post_processing(train_df, y_train, test_df, final_predictions)\n",
    "    \n",
    "    submission = pd.DataFrame({'id': test_ids, 'pollution_value': final_predictions})\n",
    "    \n",
    "    return submission, study, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "year = 2023\n",
    "train_df['datetime'] = pd.to_datetime(train_df['day_of_year'], format='%j', errors='coerce') \\\n",
    "                       + pd.to_timedelta(train_df['hour'], unit='h')\n",
    "train_df['datetime'] = train_df['datetime'].apply(\n",
    "    lambda dt: dt.replace(year=year) if pd.notnull(dt) else dt\n",
    ")\n",
    "train_df = train_df.sort_values(by='datetime')\n",
    "train_df = train_df.drop(columns='datetime')\n",
    "train_df.reset_index(drop=True, inplace=True) # CRUCIAL #\n",
    "\n",
    "X_train = train_df.drop('pollution_value', axis=1)\n",
    "y_train = train_df['pollution_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stage 1: Run feature generation multiple times\n",
    "# for run_id in range(1, 4):\n",
    "#     X_generated, generator = run_single_feature_generation(\n",
    "#         X_train, y_train, \n",
    "#         save_dir=\"model_rredone\", \n",
    "#         run_id=run_id,\n",
    "#         max_new_feats=2000,\n",
    "#         n_generations=2000\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENHANCED PIPELINE - BUILDING ON SUCCESS\n",
      "============================================================\n",
      "\n",
      "Loading existing feature generators from model_rredone...\n",
      "Loading 1 feature generators from model_rredone: ['model_rredone/feature_generator_1.pkl'].\n",
      "Combining features from 1 generators...\n",
      "Total features after generation: 17\n",
      "\n",
      "Optimizing hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 03:21:34,336] Using an existing study with name 'xgboost_optimization_enhanced' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: Run complete pipeline\n",
    "submission, study, best_params = enhanced_pipeline(\n",
    "    train_df, \n",
    "    test_df,\n",
    "    target_col='pollution_value',\n",
    "    n_trials=10,\n",
    "    feature_dir=\"model_rredone\"\n",
    ")\n",
    "\n",
    "submission.to_csv('submission_enhanced.csv', index=False)\n",
    "print(\"Submission saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}