{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced ML Pipeline - Modular Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from tabularaml.generate.features import FeatureGenerator\n",
    "from tabularaml.eval.scorers import Scorer\n",
    "from spatial_temporal_cv import SpatialTemporalKFold, FixedWindowTimeSeriesSplit\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_exp(y_true, y_pred):\n",
    "    return np.exp(-np.sqrt(mean_squared_error(y_true, y_pred))/100)\n",
    "\n",
    "def competition_score(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return np.exp(-rmse / 100)\n",
    "\n",
    "def create_cyclical_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if 'hour' in df_copy.columns:\n",
    "        df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['hour'] / 24.0)\n",
    "        df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['hour'] / 24.0)\n",
    "    \n",
    "    if 'day_of_week' in df_copy.columns:\n",
    "        df_copy['dow_sin'] = np.sin(2 * np.pi * df_copy['day_of_week'] / 7.0)\n",
    "        df_copy['dow_cos'] = np.cos(2 * np.pi * df_copy['day_of_week'] / 7.0)\n",
    "    \n",
    "    if 'day_of_year' in df_copy.columns:\n",
    "        df_copy['doy_sin'] = np.sin(2 * np.pi * df_copy['day_of_year'] / 365.0)\n",
    "        df_copy['doy_cos'] = np.cos(2 * np.pi * df_copy['day_of_year'] / 365.0)\n",
    "    \n",
    "    columns_to_drop = ['hour', 'day_of_week', 'day_of_year', 'month']\n",
    "    existing_columns_to_drop = [col for col in columns_to_drop if col in df_copy.columns]\n",
    "    if existing_columns_to_drop:\n",
    "        df_copy = df_copy.drop(columns=existing_columns_to_drop)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_features(X_train, y_train, X_test):\n",
    "    X_train_enhanced = X_train.copy()\n",
    "    X_test_enhanced = X_test.copy()\n",
    "    \n",
    "    coords_train = X_train[['latitude', 'longitude']].fillna(X_train[['latitude', 'longitude']].mean())\n",
    "    coords_test = X_test[['latitude', 'longitude']].fillna(X_train[['latitude', 'longitude']].mean())\n",
    "    \n",
    "    kde = KernelDensity(bandwidth=2.0, kernel='gaussian')\n",
    "    kde.fit(coords_train)\n",
    "    \n",
    "    X_train_enhanced['spatial_density'] = np.exp(kde.score_samples(coords_train))\n",
    "    X_test_enhanced['spatial_density'] = np.exp(kde.score_samples(coords_test))\n",
    "    \n",
    "    X_train_enhanced['is_rush_hour'] = X_train['hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "    X_test_enhanced['is_rush_hour'] = X_test['hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "    \n",
    "    X_train_enhanced['is_night'] = ((X_train['hour'] >= 22) | (X_train['hour'] <= 5)).astype(int)\n",
    "    X_test_enhanced['is_night'] = ((X_test['hour'] >= 22) | (X_test['hour'] <= 5)).astype(int)\n",
    "    \n",
    "    X_train_enhanced['lat_hour'] = X_train['latitude'] * X_train['hour']\n",
    "    X_test_enhanced['lat_hour'] = X_test['latitude'] * X_test['hour']\n",
    "    \n",
    "    return X_train_enhanced, X_test_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_feature_generation(X_train, y_train, save_dir=\"./features\", run_id=1, max_new_feats=2000, n_generations=2000):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    splitter = SpatialTemporalKFold(n_splits=5, spatial_clusters=30, temporal_clusters=10, random_state=42)\n",
    "    rmse_exp_scorer = Scorer(name=\"rmse_exp\", scorer=rmse_exp, greater_is_better=True, extra_params={}, from_probs=False)\n",
    "    \n",
    "    print(f\"Running feature generation run {run_id}...\")\n",
    "    \n",
    "    generator = FeatureGenerator(\n",
    "        task=\"regression\",\n",
    "        scorer=rmse_exp_scorer,\n",
    "        max_new_feats=max_new_feats,\n",
    "        cv=splitter,\n",
    "        n_generations=n_generations,\n",
    "        save_path=f\"{save_dir}/feature_generator_run_{run_id}.pkl\",\n",
    "    )\n",
    "    \n",
    "    results = generator.search(X_train, y_train)\n",
    "    X_generated = generator.transform(X_train)\n",
    "    \n",
    "    print(f\"Run {run_id} complete: {X_generated.shape[1]} features generated\")\n",
    "    return X_generated, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_feature_generators(feature_dir):\n",
    "    generator_files = glob.glob(os.path.join(feature_dir, \"*.pkl\"))\n",
    "    if not generator_files:\n",
    "        raise ValueError(f\"No feature generator files found in {feature_dir}\")\n",
    "    \n",
    "    generators = []\n",
    "    print(f\"Loading {len(generator_files)} feature generators from {feature_dir}\")\n",
    "    \n",
    "    for file_path in sorted(generator_files):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            generator = pickle.load(f)\n",
    "        generators.append(generator)\n",
    "    \n",
    "    return generators\n",
    "\n",
    "def combine_feature_generators(generators, X_train, X_test=None):\n",
    "    print(f\"Combining features from {len(generators)} generators...\")\n",
    "    \n",
    "    all_features_train = []\n",
    "    all_features_test = [] if X_test is not None else None\n",
    "    \n",
    "    for i, generator in enumerate(generators):\n",
    "        X_gen_train = generator.transform(X_train)\n",
    "        all_features_train.append(X_gen_train)\n",
    "        \n",
    "        if X_test is not None:\n",
    "            X_gen_test = generator.transform(X_test)\n",
    "            all_features_test.append(X_gen_test)\n",
    "    \n",
    "    X_combined_train = pd.concat(all_features_train, axis=1)\n",
    "    X_combined_train = X_combined_train.loc[:, ~X_combined_train.columns.duplicated()]\n",
    "    \n",
    "    if X_test is not None:\n",
    "        X_combined_test = pd.concat(all_features_test, axis=1)\n",
    "        X_combined_test = X_combined_test.loc[:, ~X_combined_test.columns.duplicated()]\n",
    "        return X_combined_train, X_combined_test\n",
    "    \n",
    "    return X_combined_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_objective(trial, X_base, y_train, tss, n_clusters_range=(20, 50)):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 30),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.3, 1.0),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.3, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 100.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 100.0, log=True),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 0, 2000),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'max_bin': trial.suggest_int('max_bin', 32, 512),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'enable_categorical': True,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    n_clusters = trial.suggest_int('n_clusters', n_clusters_range[0], n_clusters_range[1])\n",
    "    \n",
    "    X_base_cyclic = create_cyclical_features(X_base.copy())\n",
    "    y = np.log1p(y_train.copy())\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(tss.split(X_base_cyclic, y)):\n",
    "        X_train_fold = X_base_cyclic.iloc[train_idx].copy()\n",
    "        X_valid_fold = X_base_cyclic.iloc[valid_idx].copy()\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_valid_fold = y.iloc[valid_idx]\n",
    "        \n",
    "        if 'latitude' in X_train_fold.columns and 'longitude' in X_train_fold.columns:\n",
    "            lat_mean = X_train_fold['latitude'].mean()\n",
    "            lon_mean = X_train_fold['longitude'].mean()\n",
    "            \n",
    "            train_coords_temp = X_train_fold[['latitude', 'longitude']].copy()\n",
    "            valid_coords_temp = X_valid_fold[['latitude', 'longitude']].copy()\n",
    "            \n",
    "            train_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "            train_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "            valid_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "            valid_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "            kmeans.fit(train_coords_temp)\n",
    "            \n",
    "            X_train_fold['cluster'] = kmeans.predict(train_coords_temp)\n",
    "            X_valid_fold['cluster'] = kmeans.predict(valid_coords_temp)\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train_fold, y_train_fold, eval_set=[(X_valid_fold, y_valid_fold)], early_stopping_rounds=50, verbose=False)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_valid_fold)\n",
    "        y_pred_orig_scale = np.expm1(y_pred_fold)\n",
    "        y_valid_orig_scale = np.expm1(y_valid_fold)\n",
    "        \n",
    "        exp_score = competition_score(y_valid_orig_scale, y_pred_orig_scale)\n",
    "        fold_scores.append(exp_score)\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(X_train, y_train, params, model_type='xgb'):\n",
    "    X_train_cyclic = create_cyclical_features(X_train.copy())\n",
    "    \n",
    "    n_clusters = params.get('n_clusters', 30)\n",
    "    if 'latitude' in X_train_cyclic.columns and 'longitude' in X_train_cyclic.columns:\n",
    "        lat_mean = X_train_cyclic['latitude'].mean()\n",
    "        lon_mean = X_train_cyclic['longitude'].mean()\n",
    "        \n",
    "        coords_temp = X_train_cyclic[['latitude', 'longitude']].copy()\n",
    "        coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "        coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "        kmeans.fit(coords_temp)\n",
    "        X_train_cyclic['cluster'] = kmeans.predict(coords_temp)\n",
    "    \n",
    "    y_train_log = np.log1p(y_train)\n",
    "    \n",
    "    if model_type == 'xgb':\n",
    "        model_params = {k: v for k, v in params.items() if k != 'n_clusters'}\n",
    "        model = xgb.XGBRegressor(**model_params)\n",
    "        model.fit(X_train_cyclic, y_train_log)\n",
    "    elif model_type == 'lgb':\n",
    "        lgb_params = {\n",
    "            'n_estimators': params.get('n_estimators', 1000),\n",
    "            'learning_rate': params.get('learning_rate', 0.05),\n",
    "            'num_leaves': 2 ** params.get('max_depth', 6) - 1,\n",
    "            'feature_fraction': params.get('colsample_bytree', 0.8),\n",
    "            'bagging_fraction': params.get('subsample', 0.8),\n",
    "            'bagging_freq': 1,\n",
    "            'lambda_l1': params.get('reg_alpha', 0),\n",
    "            'lambda_l2': params.get('reg_lambda', 1),\n",
    "            'min_data_in_leaf': int(params.get('min_child_weight', 1)),\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**lgb_params)\n",
    "        model.fit(X_train_cyclic, y_train_log, verbose=-1)\n",
    "    \n",
    "    model.kmeans_ = kmeans if 'kmeans' in locals() else None\n",
    "    model.lat_mean_ = lat_mean if 'lat_mean' in locals() else None\n",
    "    model.lon_mean_ = lon_mean if 'lon_mean' in locals() else None\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ensemble(X_train, y_train, X_test, best_params, tss):\n",
    "    predictions = []\n",
    "    \n",
    "    print(\"Training XGBoost with best parameters...\")\n",
    "    xgb_model = train_final_model(X_train, y_train, best_params, model_type='xgb')\n",
    "    pred_xgb = xgb_model.predict(X_test)\n",
    "    predictions.append(('xgb_best', pred_xgb, 0.7))\n",
    "    \n",
    "    print(\"Training LightGBM...\")\n",
    "    lgb_model = train_final_model(X_train, y_train, best_params, model_type='lgb')\n",
    "    pred_lgb = lgb_model.predict(X_test)\n",
    "    predictions.append(('lgb', pred_lgb, 0.2))\n",
    "    \n",
    "    print(\"Training XGBoost variant...\")\n",
    "    xgb_variant_params = best_params.copy()\n",
    "    xgb_variant_params['max_depth'] = min(best_params['max_depth'] + 2, 15)\n",
    "    xgb_variant_params['learning_rate'] = best_params['learning_rate'] * 0.8\n",
    "    xgb_variant_model = train_final_model(X_train, y_train, xgb_variant_params, model_type='xgb')\n",
    "    pred_xgb_variant = xgb_variant_model.predict(X_test)\n",
    "    predictions.append(('xgb_variant', pred_xgb_variant, 0.1))\n",
    "    \n",
    "    final_pred = np.zeros(len(X_test))\n",
    "    for name, pred, weight in predictions:\n",
    "        final_pred += weight * pred\n",
    "    \n",
    "    return final_pred, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_post_processing(train_df, y_train, test_df, predictions):\n",
    "    train_quantiles = np.percentile(y_train, [10, 25, 50, 75, 90])\n",
    "    pred_quantiles = np.percentile(predictions, [10, 25, 50, 75, 90])\n",
    "    \n",
    "    if abs(train_quantiles[2] - pred_quantiles[2]) > 0.1 * train_quantiles[2]:\n",
    "        scale = train_quantiles[2] / pred_quantiles[2]\n",
    "        predictions_adjusted = predictions * scale\n",
    "    else:\n",
    "        predictions_adjusted = predictions\n",
    "    \n",
    "    test_lat_min, test_lat_max = test_df['latitude'].min(), test_df['latitude'].max()\n",
    "    train_lat_min, train_lat_max = train_df['latitude'].min(), train_df['latitude'].max()\n",
    "    \n",
    "    if test_lat_min < train_lat_min or test_lat_max > train_lat_max:\n",
    "        extreme_mask = ((test_df['latitude'] < train_lat_min) | (test_df['latitude'] > train_lat_max))\n",
    "        if extreme_mask.any():\n",
    "            mean_pred = predictions_adjusted[~extreme_mask].mean()\n",
    "            predictions_adjusted[extreme_mask] = (0.7 * predictions_adjusted[extreme_mask] + 0.3 * mean_pred)\n",
    "    \n",
    "    return predictions_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_pipeline(train_df, test_df, target_col='pollution_value', n_trials=1000, feature_dir=None):\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENHANCED PIPELINE - BUILDING ON SUCCESS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X_train = train_df.drop(target_col, axis=1)\n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df.copy()\n",
    "    \n",
    "    test_ids = test_df['id'].values\n",
    "    \n",
    "    print(\"\\nApplying enhanced features...\")\n",
    "    X_train_enhanced, X_test_enhanced = create_enhanced_features(X_train, y_train, X_test)\n",
    "    \n",
    "    if feature_dir and os.path.exists(feature_dir):\n",
    "        print(f\"\\nLoading existing feature generators from {feature_dir}...\")\n",
    "        generators = load_all_feature_generators(feature_dir)\n",
    "        X_train_generated, X_test_generated = combine_feature_generators(generators, X_train_enhanced, X_test_enhanced)\n",
    "    else:\n",
    "        print(\"\\nNo existing feature generators found. Please run Stage 1 first.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"Total features after generation: {X_train_generated.shape[1]}\")\n",
    "    \n",
    "    print(\"\\nOptimizing hyperparameters...\")\n",
    "    \n",
    "    tss = FixedWindowTimeSeriesSplit(n_splits=5, test_size=2700, gap=0, min_train_size=2700)\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(multivariate=True, group=True, n_startup_trials=20, constant_liar=True, seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=\"xgboost_optimization_enhanced\", sampler=sampler, storage=\"sqlite:///xgb_optuna_enhanced.db\", load_if_exists=True)\n",
    "    \n",
    "    objective_func = lambda trial: enhanced_objective(trial, X_train_generated, y_train, tss, n_clusters_range=(20, 50))\n",
    "    study.optimize(objective_func, n_trials=n_trials)\n",
    "    \n",
    "    best_params = study.best_params.copy()\n",
    "    print(f\"\\nBest score: {study.best_value:.4f}\")\n",
    "    \n",
    "    print(\"\\nGenerating final predictions...\")\n",
    "    final_predictions, model_predictions = create_model_ensemble(X_train_generated, y_train, X_test_generated, best_params, tss)\n",
    "    \n",
    "    print(\"\\nApplying post-processing...\")\n",
    "    final_predictions = np.maximum(final_predictions, 0)\n",
    "    final_predictions = advanced_post_processing(train_df, y_train, test_df, final_predictions)\n",
    "    \n",
    "    submission = pd.DataFrame({'id': test_ids, 'pollution_value': final_predictions})\n",
    "    \n",
    "    return submission, study, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_train = train_df.drop('pollution_value', axis=1)\n",
    "y_train = train_df['pollution_value']\n",
    "\n",
    "# Apply enhanced features\n",
    "X_train_enhanced, X_test_enhanced = create_enhanced_features(X_train, y_train, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Run feature generation multiple times\n",
    "for run_id in range(1, 4):\n",
    "    X_generated, generator = run_single_feature_generation(\n",
    "        X_train_enhanced, y_train, \n",
    "        save_dir=\"./features\", \n",
    "        run_id=run_id,\n",
    "        max_new_feats=2000,\n",
    "        n_generations=2000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Run complete pipeline\n",
    "submission, study, best_params = enhanced_pipeline(\n",
    "    train_df, \n",
    "    test_df,\n",
    "    target_col='pollution_value',\n",
    "    n_trials=1000,\n",
    "    feature_dir=\"./features\"\n",
    ")\n",
    "\n",
    "submission.to_csv('submission_enhanced.csv', index=False)\n",
    "print(\"Submission saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}