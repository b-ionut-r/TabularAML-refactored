{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from spatio_temporal import (\n",
    "    AdvancedSpatioTemporalFeatures, \n",
    "    SpatioTemporalCV,\n",
    "    TemporalDomainAdaptation,\n",
    "    SpatioTemporalDistributionAnalyzer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading data...\n",
      "Train shape: (7649, 8)\n",
      "Test shape: (2739, 7)\n",
      "Missing values: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Loading data...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Quick data quality check\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Missing values: {train_df.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Analyzing distribution shifts...\n",
      "Analyzing distribution differences...\n",
      "\n",
      "Spatial Distribution:\n",
      "  Latitude KS: 0.3457 (p=1.52e-214)\n",
      "  Longitude KS: 0.2957 (p=3.77e-156)\n",
      "\n",
      "Temporal Distribution Shifts:\n",
      "  hour: Wasserstein=3.3287 (significant, p=1.29e-252)\n",
      "  day_of_week: Wasserstein=1.8408 (significant, p=4.61e-321)\n",
      "  month: Wasserstein=6.1064 (significant, p=0.00e+00)\n",
      "  day_of_year: Wasserstein=175.2113 (significant, p=0.00e+00)\n",
      "Significant shifts detected in:\n",
      "  - hour (Wasserstein distance: 3.329)\n",
      "  - day_of_week (Wasserstein distance: 1.841)\n",
      "  - month (Wasserstein distance: 6.106)\n",
      "  - day_of_year (Wasserstein distance: 175.211)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2: Analyzing distribution shifts...\")\n",
    "analyzer = SpatioTemporalDistributionAnalyzer()\n",
    "spatial_stats, temporal_stats = analyzer.analyze(train_df, test_df)\n",
    "\n",
    "# Quick summary\n",
    "print(\"Significant shifts detected in:\")\n",
    "for feature, stats in temporal_stats.items():\n",
    "    if stats['ks_pvalue'] < 0.05:\n",
    "        print(f\"  - {feature} (Wasserstein distance: {stats['wasserstein_distance']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Preparing features...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 3: Preparing features...\")\n",
    "X_train = train_df.drop('pollution_value', axis=1)\n",
    "y_train = train_df['pollution_value']\n",
    "X_test = test_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Engineering features for validation...\n",
      "Features for validation: 44\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 4: Engineering features for validation...\")\n",
    "\n",
    "# Initialize with row_only=True to prevent leakage\n",
    "fe_validator = AdvancedSpatioTemporalFeatures(\n",
    "    row_only=True,  # IMPORTANT: Prevents leakage during CV\n",
    "    n_spatial_clusters=20,\n",
    "    n_temporal_clusters=10,\n",
    "    use_distribution_matching=True,\n",
    "    test_distribution=temporal_stats\n",
    ")\n",
    "\n",
    "# Transform for validation\n",
    "X_train_val = fe_validator.fit_transform(X_train, y_train)\n",
    "print(f\"Features for validation: {X_train_val.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Setting up spatio-temporal aware CV...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 5: Setting up spatio-temporal aware CV...\")\n",
    "\n",
    "cv = SpatioTemporalCV(\n",
    "    n_splits=5,\n",
    "    test_spatial_coords=test_df[['latitude', 'longitude']].values,\n",
    "    test_temporal_features=test_df[['hour', 'month', 'day_of_week', 'day_of_year']],\n",
    "    spatial_weight=0.5,  # Balance between spatial and temporal matching\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Model validation with distribution-aware CV...\n",
      "\n",
      "Validating lightgbm...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgbm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_tr_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_tr_scaled, y_tr)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 6: Model validation with distribution-aware CV...\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'lightgbm': lgb.LGBMRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'xgboost': xgb.XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'random_forest': RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Validate each model\n",
    "cv_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nValidating {model_name}...\")\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_val, y_train)):\n",
    "        # Split data\n",
    "        X_tr, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Train model\n",
    "        if model_name in ['lightgbm', 'xgboost']:\n",
    "            model.fit(\n",
    "                X_tr_scaled, y_tr,\n",
    "                eval_set=[(X_val_scaled, y_val)],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_tr_scaled, y_tr)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        fold_scores.append(rmse)\n",
    "        \n",
    "        print(f\"  Fold {fold + 1}: RMSE = {rmse:.4f}\")\n",
    "    \n",
    "    avg_score = np.mean(fold_scores)\n",
    "    cv_results[model_name] = {\n",
    "        'mean_rmse': avg_score,\n",
    "        'std_rmse': np.std(fold_scores),\n",
    "        'scores': fold_scores\n",
    "    }\n",
    "    print(f\"  Average: {avg_score:.4f} (Â±{np.std(fold_scores):.4f})\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = min(cv_results.keys(), key=lambda x: cv_results[x]['mean_rmse'])\n",
    "print(f\"\\nBest model: {best_model_name} (RMSE: {cv_results[best_model_name]['mean_rmse']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7: Training final model with all features...\n",
      "Domain adaptation weights range: [0.24, 23.80]\n",
      "Final feature count: 61\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 7: Training final model with all features...\")\n",
    "\n",
    "# Create feature engineer without restrictions\n",
    "fe_final = AdvancedSpatioTemporalFeatures(\n",
    "    row_only=False,  # Use ALL features including cluster-based ones\n",
    "    n_spatial_clusters=30,  # More clusters for final model\n",
    "    n_temporal_clusters=15,\n",
    "    use_distribution_matching=True,\n",
    "    test_distribution=temporal_stats\n",
    ")\n",
    "\n",
    "# Apply domain adaptation weights\n",
    "domain_adapter = TemporalDomainAdaptation(method='importance_weighting')\n",
    "domain_adapter.fit(X_train, X_test)\n",
    "sample_weights = domain_adapter.get_weights()\n",
    "\n",
    "print(f\"Domain adaptation weights range: [{sample_weights.min():.2f}, {sample_weights.max():.2f}]\")\n",
    "\n",
    "# Transform with all features\n",
    "X_train_final = fe_final.fit_transform(X_train, y_train)\n",
    "X_test_final = fe_final.transform(X_test)\n",
    "\n",
    "print(f\"Final feature count: {X_train_final.shape[1]}\")\n",
    "\n",
    "# Scale\n",
    "scaler_final = StandardScaler()\n",
    "X_train_scaled = scaler_final.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler_final.transform(X_test_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 8: Training ensemble of best models...\n",
      "\n",
      "Ensemble weights: {}\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnsemble weights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m([m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mm\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtop_models],\u001b[38;5;250m \u001b[39mensemble_weights))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Weighted average\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensemble_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ml_env\\lib\\site-packages\\numpy\\lib\\function_base.py:550\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[0;32m    548\u001b[0m     scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw)\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m--> 550\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    551\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    553\u001b[0m     avg \u001b[38;5;241m=\u001b[39m avg_as_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(a, wgt,\n\u001b[0;32m    554\u001b[0m                       dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\u001b[38;5;241m.\u001b[39msum(axis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw) \u001b[38;5;241m/\u001b[39m scl\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 8: Training ensemble of best models...\")\n",
    "\n",
    "# Use top 3 models from validation\n",
    "top_models = sorted(cv_results.items(), key=lambda x: x[1]['mean_rmse'])[:3]\n",
    "ensemble_predictions = []\n",
    "ensemble_weights = []\n",
    "\n",
    "for model_name, scores in top_models:\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Train with sample weights\n",
    "    if model_name in ['lightgbm', 'xgboost']:\n",
    "        model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    ensemble_predictions.append(predictions)\n",
    "    \n",
    "    # Weight inversely proportional to CV error\n",
    "    weight = 1.0 / scores['mean_rmse']\n",
    "    ensemble_weights.append(weight)\n",
    "\n",
    "# Normalize weights\n",
    "ensemble_weights = np.array(ensemble_weights)\n",
    "ensemble_weights = ensemble_weights / ensemble_weights.sum()\n",
    "\n",
    "print(f\"\\nEnsemble weights: {dict(zip([m[0] for m in top_models], ensemble_weights))}\")\n",
    "\n",
    "# Weighted average\n",
    "final_predictions = np.average(ensemble_predictions, axis=0, weights=ensemble_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 9: Creating submission...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 9: Creating submission...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Post-processing\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[43mfinal_predictions\u001b[49m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Ensure non-negative\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Check prediction statistics\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrediction statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 9: Creating submission...\")\n",
    "\n",
    "# Post-processing\n",
    "final_predictions = np.maximum(final_predictions, 0)  # Ensure non-negative\n",
    "\n",
    "# Check prediction statistics\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"  Mean: {final_predictions.mean():.4f}\")\n",
    "print(f\"  Std: {final_predictions.std():.4f}\")\n",
    "print(f\"  Min: {final_predictions.min():.4f}\")\n",
    "print(f\"  Max: {final_predictions.max():.4f}\")\n",
    "\n",
    "# Compare to training target distribution\n",
    "print(f\"\\nTraining target statistics:\")\n",
    "print(f\"  Mean: {y_train.mean():.4f}\")\n",
    "print(f\"  Std: {y_train.std():.4f}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'pollution_value': final_predictions\n",
    "})\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission_advanced.csv', index=False)\n",
    "print(\"\\nSubmission saved to 'submission_advanced.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10: Analyzing feature importance...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 10: Analyzing feature importance...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get feature importance from best model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbest_model_name\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgbm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Retrain single model for feature importance\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m models[best_model_name]\n\u001b[0;32m      7\u001b[0m     best_model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train, sample_weight\u001b[38;5;241m=\u001b[39msample_weights, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model_name' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 10: Analyzing feature importance...\")\n",
    "\n",
    "# Get feature importance from best model\n",
    "if best_model_name == 'lightgbm':\n",
    "    # Retrain single model for feature importance\n",
    "    best_model = models[best_model_name]\n",
    "    best_model.fit(X_train_scaled, y_train, sample_weight=sample_weights, verbose=False)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train_final.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 most important features:\")\n",
    "    for idx, row in feature_importance.head(15).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPLETE! Summary of what we did:\n",
      "============================================================\n",
      "1. Analyzed distribution shifts between train and test\n",
      "2. Created distribution-aware features\n",
      "3. Used custom CV that matches test distribution\n",
      "4. Applied domain adaptation weights\n",
      "5. Trained ensemble of best models\n",
      "6. Post-processed predictions\n",
      "\n",
      "Key innovations:\n",
      "- Spatial clustering features for location generalization\n",
      "- Temporal pattern clustering\n",
      "- Distribution-matching CV strategy\n",
      "- Domain adaptation for temporal shifts\n",
      "- Two-stage feature engineering (safe for CV + full for final)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE! Summary of what we did:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Analyzed distribution shifts between train and test\")\n",
    "print(\"2. Created distribution-aware features\")\n",
    "print(\"3. Used custom CV that matches test distribution\")\n",
    "print(\"4. Applied domain adaptation weights\")\n",
    "print(\"5. Trained ensemble of best models\")\n",
    "print(\"6. Post-processed predictions\")\n",
    "print(\"\\nKey innovations:\")\n",
    "print(\"- Spatial clustering features for location generalization\")\n",
    "print(\"- Temporal pattern clustering\")\n",
    "print(\"- Distribution-matching CV strategy\")\n",
    "print(\"- Domain adaptation for temporal shifts\")\n",
    "print(\"- Two-stage feature engineering (safe for CV + full for final)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TIPS FOR FURTHER IMPROVEMENT:\n",
      "============================================================\n",
      "\n",
      "1. Hyperparameter tuning:\n",
      "   - Use Optuna with the custom CV\n",
      "   - Focus on regularization parameters\n",
      "   \n",
      "2. More sophisticated features:\n",
      "   - Voronoi tessellation features\n",
      "   - Kriging/spatial interpolation\n",
      "   - Weather API data if allowed\n",
      "   \n",
      "3. Advanced models:\n",
      "   - Neural networks with spatial/temporal embeddings\n",
      "   - Gradient boosting with custom objectives\n",
      "   - Hierarchical models\n",
      "   \n",
      "4. Ensemble strategies:\n",
      "   - Stacking with out-of-fold predictions\n",
      "   - Bayesian model averaging\n",
      "   - Rank averaging for robustness\n",
      "   \n",
      "5. Post-processing:\n",
      "   - Isotonic regression calibration\n",
      "   - Outlier detection and clipping\n",
      "   - Test-time augmentation\n",
      "\n",
      "Remember: The key is handling the distribution shift properly!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIPS FOR FURTHER IMPROVEMENT:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. Hyperparameter tuning:\n",
    "   - Use Optuna with the custom CV\n",
    "   - Focus on regularization parameters\n",
    "   \n",
    "2. More sophisticated features:\n",
    "   - Voronoi tessellation features\n",
    "   - Kriging/spatial interpolation\n",
    "   - Weather API data if allowed\n",
    "   \n",
    "3. Advanced models:\n",
    "   - Neural networks with spatial/temporal embeddings\n",
    "   - Gradient boosting with custom objectives\n",
    "   - Hierarchical models\n",
    "   \n",
    "4. Ensemble strategies:\n",
    "   - Stacking with out-of-fold predictions\n",
    "   - Bayesian model averaging\n",
    "   - Rank averaging for robustness\n",
    "   \n",
    "5. Post-processing:\n",
    "   - Isotonic regression calibration\n",
    "   - Outlier detection and clipping\n",
    "   - Test-time augmentation\n",
    "\n",
    "Remember: The key is handling the distribution shift properly!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
