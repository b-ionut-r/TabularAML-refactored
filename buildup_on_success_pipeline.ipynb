{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q numpy scikit-learn pandas xgboost lightgbm category_encoders matplotlib seaborn cloudpickle shap optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced ML Pipeline - Modular Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from tabularaml.generate.features import FeatureGenerator\n",
    "from tabularaml.eval.scorers import Scorer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Generator, Tuple, Optional, Union\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.validation import check_X_y, indexable\n",
    "from sklearn.utils import check_random_state\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "\n",
    "\n",
    "class SpatialTemporalKFold(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Spatial-Temporal Cross-Validator for geographic time-series data.\n",
    "    \n",
    "    This cross-validator creates folds that respect both spatial and temporal \n",
    "    dependencies in the data, preventing data leakage in spatial-temporal \n",
    "    prediction tasks like air pollution forecasting.\n",
    "    \n",
    "    The strategy:\n",
    "    1. Creates spatial clusters using geographic coordinates (lat/lon).\n",
    "    2. Creates temporal clusters using cyclical time features.\n",
    "    3. Combines spatial-temporal groups to ensure validation sets are \n",
    "       spatially and temporally separated from training sets.\n",
    "    4. Optionally stratifies by target variable ranges, with an option to\n",
    "       use a log transform for skewed targets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of cross-validation folds.\n",
    "    spatial_clusters : int, default=20\n",
    "        Number of spatial clusters for geographic grouping.\n",
    "    temporal_clusters : int, default=8\n",
    "        Number of temporal clusters for time-based grouping.\n",
    "    lat_col : str, default='latitude'\n",
    "        Column name for latitude coordinates.\n",
    "    lon_col : str, default='longitude' \n",
    "        Column name for longitude coordinates.\n",
    "    time_cols : dict, default=None\n",
    "        Dictionary mapping time column names to their cycles.\n",
    "    stratify : bool, default=True\n",
    "        Whether to stratify splits by target variable quantiles.\n",
    "    stratify_log_transform : bool, default=True\n",
    "        If True, applies a log transform (log1p) to the target variable\n",
    "        before stratification. Ideal for skewed targets.\n",
    "    n_quantiles : int, default=5\n",
    "        Number of quantiles for stratification (if stratify=True).\n",
    "    random_state : int, default=None\n",
    "        Random state for reproducible splits.\n",
    "    shuffle : bool, default=True\n",
    "        Whether to shuffle data before splitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_splits: int = 5,\n",
    "                 spatial_clusters: int = 20,\n",
    "                 temporal_clusters: int = 8,\n",
    "                 lat_col: str = 'latitude',\n",
    "                 lon_col: str = 'longitude',\n",
    "                 time_cols: Optional[dict] = None,\n",
    "                 stratify: bool = True,\n",
    "                 stratify_log_transform: bool = True,\n",
    "                 n_quantiles: int = 5,\n",
    "                 random_state: Optional[int] = None,\n",
    "                 shuffle: bool = True):\n",
    "        \n",
    "        self.n_splits = n_splits\n",
    "        self.spatial_clusters = spatial_clusters\n",
    "        self.temporal_clusters = temporal_clusters\n",
    "        self.lat_col = lat_col\n",
    "        self.lon_col = lon_col\n",
    "        self.time_cols = time_cols or {\n",
    "            'day_of_year': 365, 'hour': 24, 'day_of_week': 7, 'month': 12\n",
    "        }\n",
    "        self.stratify = stratify\n",
    "        self.stratify_log_transform = stratify_log_transform\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Validation\n",
    "        if n_splits < 2:\n",
    "            raise ValueError(\"n_splits must be at least 2\")\n",
    "        if spatial_clusters < n_splits:\n",
    "            warnings.warn(f\"spatial_clusters ({spatial_clusters}) < n_splits ({n_splits}). \"\n",
    "                         \"This may result in poor spatial separation.\")\n",
    "        if temporal_clusters < 2:\n",
    "            raise ValueError(\"temporal_clusters must be at least 2\")\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Returns the number of splitting iterations.\"\"\"\n",
    "        return self.n_splits\n",
    "    \n",
    "    def _validate_data(self, X: Union[pd.DataFrame, np.ndarray], \n",
    "                      y: Optional[Union[pd.Series, np.ndarray]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Validate and convert input data to DataFrame.\"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            if isinstance(X, np.ndarray):\n",
    "                expected_cols = [self.lat_col, self.lon_col] + list(self.time_cols.keys())\n",
    "                if X.shape[1] >= len(expected_cols):\n",
    "                    X = pd.DataFrame(X, columns=expected_cols[:X.shape[1]])\n",
    "                else:\n",
    "                    raise ValueError(f\"Expected at least {len(expected_cols)} columns, got {X.shape[1]}\")\n",
    "            else:\n",
    "                raise TypeError(\"X must be pandas DataFrame or numpy array\")\n",
    "        \n",
    "        missing_cols = [col for col in [self.lat_col, self.lon_col] + list(self.time_cols.keys()) if col not in X.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def _create_spatial_clusters(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create spatial clusters using K-means on lat/lon coordinates.\"\"\"\n",
    "        coords = X[[self.lat_col, self.lon_col]]\n",
    "        n_clusters = min(self.spatial_clusters, len(coords))\n",
    "        if n_clusters < self.spatial_clusters:\n",
    "            warnings.warn(f\"Reducing spatial_clusters from {self.spatial_clusters} to {n_clusters} due to insufficient data points.\")\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init=10)\n",
    "        return kmeans.fit_predict(coords.copy().fillna(coords.mean()).values)\n",
    "    \n",
    "    def _create_temporal_features(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Convert cyclical time features to circular coordinates.\"\"\"\n",
    "        temporal_features = []\n",
    "        for col, cycle_length in self.time_cols.items():\n",
    "            if col in X.columns:\n",
    "                radians = 2 * np.pi * X[col] / cycle_length\n",
    "                temporal_features.extend([np.sin(radians), np.cos(radians)])\n",
    "        \n",
    "        return np.column_stack(temporal_features) if temporal_features else np.zeros((len(X), 0))\n",
    "    \n",
    "    def _create_temporal_clusters(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create temporal clusters using cyclical time features.\"\"\"\n",
    "        temporal_coords = self._create_temporal_features(X)\n",
    "        if temporal_coords.shape[1] == 0:\n",
    "            return np.zeros(len(X), dtype=int)\n",
    "            \n",
    "        n_clusters = min(self.temporal_clusters, len(temporal_coords))\n",
    "        if n_clusters < self.temporal_clusters:\n",
    "            warnings.warn(f\"Reducing temporal_clusters from {self.temporal_clusters} to {n_clusters} due to insufficient data points.\")\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init=10)\n",
    "        return kmeans.fit_predict(temporal_coords)\n",
    "    \n",
    "    def _create_stratification_groups(self, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create stratification groups based on target variable quantiles.\"\"\"\n",
    "        if not self.stratify or y is None:\n",
    "            return np.zeros(len(y), dtype=int)\n",
    "        \n",
    "        y_stratify = y.copy()\n",
    "        if self.stratify_log_transform:\n",
    "            y_stratify = np.log1p(y_stratify)\n",
    "        \n",
    "        quantiles = np.linspace(0, 1, self.n_quantiles + 1)\n",
    "        quantile_values = np.quantile(y_stratify, quantiles)\n",
    "        \n",
    "        return np.digitize(y_stratify, quantile_values[1:-1])\n",
    "    \n",
    "    def _create_combined_groups(self, X: pd.DataFrame, y: Optional[np.ndarray] = None) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"Create combined spatial-temporal-stratification groups.\"\"\"\n",
    "        spatial_labels = self._create_spatial_clusters(X)\n",
    "        temporal_labels = self._create_temporal_clusters(X)\n",
    "        strat_labels = self._create_stratification_groups(y) if y is not None else np.zeros(len(X), dtype=int)\n",
    "        \n",
    "        max_spatial = np.max(spatial_labels) + 1\n",
    "        max_temporal = np.max(temporal_labels) + 1\n",
    "        max_strat = np.max(strat_labels) + 1\n",
    "        \n",
    "        combined_groups = (spatial_labels * max_temporal * max_strat + \n",
    "                           temporal_labels * max_strat + \n",
    "                           strat_labels)\n",
    "        \n",
    "        metadata = {\n",
    "            'spatial_labels': spatial_labels,\n",
    "            'temporal_labels': temporal_labels,\n",
    "            'strat_labels': strat_labels,\n",
    "        }\n",
    "        \n",
    "        return combined_groups, metadata\n",
    "    \n",
    "    def split(self, X: Union[pd.DataFrame, np.ndarray], \n",
    "              y: Optional[Union[pd.Series, np.ndarray]] = None, \n",
    "              groups: Optional[np.ndarray] = None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "        \"\"\"\n",
    "        Generate indices to split data into training and test set.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X, y)\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        \n",
    "        if y is not None:\n",
    "            y = np.asarray(y)\n",
    "            \n",
    "        rng = check_random_state(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            rng.shuffle(indices)\n",
    "            X_shuffled = X.iloc[indices].reset_index(drop=True)\n",
    "            y_shuffled = y[indices] if y is not None else None\n",
    "        else:\n",
    "            X_shuffled, y_shuffled = X, y\n",
    "\n",
    "        combined_groups, _ = self._create_combined_groups(X_shuffled, y_shuffled)\n",
    "        unique_groups = np.unique(combined_groups)\n",
    "        \n",
    "        if len(unique_groups) < self.n_splits:\n",
    "            raise ValueError(f\"Cannot create {self.n_splits} splits with only \"\n",
    "                             f\"{len(unique_groups)} unique groups. \"\n",
    "                             f\"Consider reducing n_splits or clustering parameters.\")\n",
    "        \n",
    "        rng.shuffle(unique_groups)\n",
    "        \n",
    "        fold_groups = [[] for _ in range(self.n_splits)]\n",
    "        for i, group in enumerate(unique_groups):\n",
    "            fold_groups[i % self.n_splits].append(group)\n",
    "        \n",
    "        for fold_idx in range(self.n_splits):\n",
    "            test_groups = np.array(fold_groups[fold_idx])\n",
    "            test_mask = np.isin(combined_groups, test_groups)\n",
    "            train_mask = ~test_mask\n",
    "            \n",
    "            original_test_indices = indices[test_mask]\n",
    "            original_train_indices = indices[train_mask]\n",
    "            \n",
    "            if len(original_test_indices) == 0 or len(original_train_indices) == 0:\n",
    "                warnings.warn(f\"Fold {fold_idx} has an empty train/test set, skipping.\")\n",
    "                continue\n",
    "                \n",
    "            yield original_train_indices, original_test_indices\n",
    "\n",
    "\n",
    "class StratifiedSpatialTemporalKFold(SpatialTemporalKFold):\n",
    "    \"\"\"\n",
    "    A stratified version of SpatialTemporalKFold.\n",
    "    \n",
    "    This class enforces stratification by requiring the target variable `y` \n",
    "    and sets `stratify=True` by default. It is ideal for regression tasks \n",
    "    with skewed target distributions where maintaining a balanced distribution \n",
    "    in each fold is critical for reliable model evaluation.\n",
    "    \n",
    "    The key enhancement is the automatic application of a log transform to\n",
    "    the target variable before creating stratification bins, which is highly\n",
    "    effective for right-skewed data like pollution values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        # Enforce stratification for this class\n",
    "        kwargs['stratify'] = True\n",
    "        # Default to log-transform for better handling of skewed targets\n",
    "        if 'stratify_log_transform' not in kwargs:\n",
    "            kwargs['stratify_log_transform'] = True\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"\n",
    "        Generate indices to split data into training and test set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame or array-like\n",
    "            Training data.\n",
    "        y : array-like\n",
    "            The target variable for stratification. Must be provided.\n",
    "        groups : array-like, optional\n",
    "            Not used, for API compatibility.\n",
    "            \n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray  \n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if y is None:\n",
    "            raise ValueError(\"StratifiedSpatialTemporalKFold requires the target variable 'y' for stratification.\")\n",
    "            \n",
    "        yield from super().split(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import BaseCrossValidator\n",
    "import numpy as np\n",
    "\n",
    "class FixedWindowTimeSeriesSplit(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Custom time-series cross-validator with fixed-size test windows.\n",
    "    Ensures every fold has meaningful training data and proper temporal ordering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int\n",
    "        Number of folds. Must be at least 1.\n",
    "    test_size : int\n",
    "        Number of samples in each test fold.\n",
    "    gap : int, default=0\n",
    "        Number of samples to exclude between train and test sets.\n",
    "    min_train_size : int, default=None\n",
    "        Minimum number of training samples required. If None, defaults to test_size.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, test_size=2700, gap=0, min_train_size=None):\n",
    "        if n_splits < 1:\n",
    "            raise ValueError(\"n_splits must be at least 1.\")\n",
    "        if test_size < 1:\n",
    "            raise ValueError(\"test_size must be at least 1.\")\n",
    "        if gap < 0:\n",
    "            raise ValueError(\"gap must be non-negative.\")\n",
    "        \n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.gap = gap\n",
    "        self.min_train_size = min_train_size or test_size\n",
    "        \n",
    "        if self.min_train_size < 1:\n",
    "            raise ValueError(\"min_train_size must be at least 1.\")\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Check if we have enough data for at least one split\n",
    "        min_required = self.min_train_size + self.gap + self.test_size\n",
    "        if min_required > n_samples:\n",
    "            raise ValueError(\n",
    "                f\"Not enough samples. Need at least {min_required} samples \"\n",
    "                f\"(min_train_size={self.min_train_size} + gap={self.gap} + test_size={self.test_size}), \"\n",
    "                f\"but got {n_samples}.\"\n",
    "            )\n",
    "        \n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        if self.n_splits == 1:\n",
    "            # Single window: place test at the end, ensure minimum training size\n",
    "            test_end = n_samples\n",
    "            test_start = test_end - self.test_size\n",
    "            train_end = test_start - self.gap\n",
    "            \n",
    "            # Ensure we have minimum training size\n",
    "            if train_end < self.min_train_size:\n",
    "                train_end = self.min_train_size\n",
    "                test_start = train_end + self.gap\n",
    "                test_end = test_start + self.test_size\n",
    "                \n",
    "                # Check if this fits within our data\n",
    "                if test_end > n_samples:\n",
    "                    raise ValueError(\n",
    "                        f\"Cannot fit single split with constraints. \"\n",
    "                        f\"Need {self.min_train_size + self.gap + self.test_size} samples, got {n_samples}.\"\n",
    "                    )\n",
    "            \n",
    "            train_idx = indices[:train_end]\n",
    "            test_idx = indices[test_start:test_end]\n",
    "            yield train_idx, test_idx\n",
    "            return\n",
    "        \n",
    "        # For multiple splits, distribute test windows\n",
    "        # Last test window ends at n_samples, work backwards\n",
    "        test_windows = []\n",
    "        \n",
    "        # Calculate positions for test windows\n",
    "        # We want to distribute them evenly in the available space\n",
    "        latest_test_end = n_samples\n",
    "        earliest_test_start = self.min_train_size + self.gap\n",
    "        \n",
    "        # Available space for test window starts\n",
    "        available_space = latest_test_end - self.test_size - earliest_test_start\n",
    "        \n",
    "        if available_space < 0:\n",
    "            raise ValueError(\n",
    "                \"Cannot create requested splits. Try reducing n_splits, test_size, or min_train_size.\"\n",
    "            )\n",
    "        \n",
    "        # Calculate step size between test windows\n",
    "        if self.n_splits == 1:\n",
    "            step = 0\n",
    "        else:\n",
    "            step = available_space / (self.n_splits - 1)\n",
    "        \n",
    "        # Generate test windows from last to first\n",
    "        for i in range(self.n_splits):\n",
    "            # Calculate test window position\n",
    "            test_start = int(earliest_test_start + i * step)\n",
    "            test_end = test_start + self.test_size\n",
    "            \n",
    "            # Ensure test window doesn't exceed data bounds\n",
    "            if test_end > n_samples:\n",
    "                test_end = n_samples\n",
    "                test_start = test_end - self.test_size\n",
    "            \n",
    "            # Calculate training end (before gap)\n",
    "            train_end = test_start - self.gap\n",
    "            \n",
    "            # Ensure minimum training size\n",
    "            if train_end < self.min_train_size:\n",
    "                raise ValueError(\n",
    "                    f\"Split {i+1} would have insufficient training data. \"\n",
    "                    f\"Try reducing n_splits or min_train_size.\"\n",
    "                )\n",
    "            \n",
    "            train_idx = indices[:train_end]\n",
    "            test_idx = indices[test_start:test_end]\n",
    "            \n",
    "            yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_exp(y_true, y_pred):\n",
    "    return np.exp(-np.sqrt(mean_squared_error(y_true, y_pred))/100)\n",
    "\n",
    "def competition_score(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return np.exp(-rmse / 100)\n",
    "\n",
    "def create_cyclical_features(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if 'hour' in df_copy.columns:\n",
    "        df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['hour'] / 24.0)\n",
    "        df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['hour'] / 24.0)\n",
    "    \n",
    "    if 'day_of_week' in df_copy.columns:\n",
    "        df_copy['dow_sin'] = np.sin(2 * np.pi * df_copy['day_of_week'] / 7.0)\n",
    "        df_copy['dow_cos'] = np.cos(2 * np.pi * df_copy['day_of_week'] / 7.0)\n",
    "    \n",
    "    if 'day_of_year' in df_copy.columns:\n",
    "        df_copy['doy_sin'] = np.sin(2 * np.pi * df_copy['day_of_year'] / 365.0)\n",
    "        df_copy['doy_cos'] = np.cos(2 * np.pi * df_copy['day_of_year'] / 365.0)\n",
    "    \n",
    "    columns_to_drop = ['hour', 'day_of_week', 'day_of_year', 'month']\n",
    "    existing_columns_to_drop = [col for col in columns_to_drop if col in df_copy.columns]\n",
    "    if existing_columns_to_drop:\n",
    "        df_copy = df_copy.drop(columns=existing_columns_to_drop)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_feature_generation(X_train, y_train, save_dir=\"./features\", run_id=1, max_new_feats=2000, n_generations=2000):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    splitter = SpatialTemporalKFold(n_splits=10, random_state=42+run_id)\n",
    "    rmse_exp_scorer = Scorer(name=\"rmse_exp\", scorer=rmse_exp, greater_is_better=True, extra_params={}, from_probs=False)\n",
    "    \n",
    "    print(f\"Running feature generation run {run_id}...\")\n",
    "    \n",
    "    generator = FeatureGenerator(\n",
    "        task=\"regression\",\n",
    "        scorer=rmse_exp_scorer,\n",
    "        max_new_feats=max_new_feats,\n",
    "        cv=splitter,\n",
    "        n_generations=n_generations,\n",
    "        save_path=f\"{save_dir}/feature_generator_run_{run_id}.pkl\",\n",
    "    )\n",
    "    \n",
    "    results = generator.search(X_train, y_train)\n",
    "    X_generated = generator.transform(X_train)\n",
    "    \n",
    "    print(f\"Run {run_id} complete: {X_generated.shape[1]} features generated\")\n",
    "    return X_generated, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_feature_generators(feature_dir):\n",
    "    generator_files = glob.glob(os.path.join(feature_dir, \"*.pkl\"))\n",
    "    if not generator_files:\n",
    "        raise ValueError(f\"No feature generator files found in {feature_dir}\")\n",
    "    \n",
    "    generators = []\n",
    "    print(f\"Loading {len(generator_files)} feature generators from {feature_dir}: {sorted(generator_files)}.\")\n",
    "    for file_path in sorted(generator_files):\n",
    "        generator = FeatureGenerator.load(file_path)\n",
    "        generators.append(generator)\n",
    "\n",
    "    return generators\n",
    "\n",
    "def combine_feature_generators(generators, X_train, X_test=None):\n",
    "    print(f\"Combining features from {len(generators)} generators...\")\n",
    "    X_train = X_train.copy()\n",
    "    if X_test is not None:\n",
    "        X_test = X_test.copy()\n",
    "    \n",
    "    for i, generator in enumerate(generators):\n",
    "        X_train = generator.fit_transform(X_train)\n",
    "        if X_test is not None:\n",
    "            X_test = generator.transform(X_test)\n",
    "    \n",
    "    if X_test is not None:\n",
    "        return X_train, X_test\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial, X_base, y_train, tss, n_clusters_range=(20, 50)):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 30),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.3, 1.0),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.3, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 100.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 100.0, log=True),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 0, 2000),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'max_bin': trial.suggest_int('max_bin', 32, 512),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'enable_categorical': True,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    n_clusters = trial.suggest_int('n_clusters', n_clusters_range[0], n_clusters_range[1])\n",
    "    \n",
    "    X_base_cyclic = create_cyclical_features(X_base.copy())\n",
    "    y = np.log1p(y_train.copy())\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(tss.split(X_base_cyclic, y)):\n",
    "        X_train_fold = X_base_cyclic.iloc[train_idx].copy()\n",
    "        X_valid_fold = X_base_cyclic.iloc[valid_idx].copy()\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_valid_fold = y.iloc[valid_idx]\n",
    "        \n",
    "        if 'latitude' in X_train_fold.columns and 'longitude' in X_train_fold.columns:\n",
    "            lat_mean = X_train_fold['latitude'].mean()\n",
    "            lon_mean = X_train_fold['longitude'].mean()\n",
    "            \n",
    "            train_coords_temp = X_train_fold[['latitude', 'longitude']].copy()\n",
    "            valid_coords_temp = X_valid_fold[['latitude', 'longitude']].copy()\n",
    "            \n",
    "            train_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "            train_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "            valid_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "            valid_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "            kmeans.fit(train_coords_temp)\n",
    "            \n",
    "            X_train_fold['cluster'] = kmeans.predict(train_coords_temp)\n",
    "            X_valid_fold['cluster'] = kmeans.predict(valid_coords_temp)\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_valid_fold)\n",
    "        y_pred_orig_scale = np.expm1(y_pred_fold)\n",
    "        y_valid_orig_scale = np.expm1(y_valid_fold)\n",
    "        \n",
    "        exp_score = competition_score(y_valid_orig_scale, y_pred_orig_scale)\n",
    "        fold_scores.append(exp_score)\n",
    "    \n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "\n",
    "\n",
    "def lgb_objective(trial, X_base, y_train, tss, n_clusters_range=(20, 50)):\n",
    "    \"\"\"Separate objective function for LightGBM optimization\"\"\"\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 300),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.3, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.3, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 20),\n",
    "        'max_bin': trial.suggest_int('max_bin', 32, 512),\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "    \n",
    "    if params['max_depth'] == -1:\n",
    "        params.pop('max_depth')\n",
    "    \n",
    "    n_clusters = trial.suggest_int('n_clusters', n_clusters_range[0], n_clusters_range[1])\n",
    "    \n",
    "    X_base_cyclic = create_cyclical_features(X_base.copy())\n",
    "    y = np.log1p(y_train.copy())\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(tss.split(X_base_cyclic, y)):\n",
    "        X_train_fold = X_base_cyclic.iloc[train_idx].copy()\n",
    "        X_valid_fold = X_base_cyclic.iloc[valid_idx].copy()\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_valid_fold = y.iloc[valid_idx]\n",
    "        \n",
    "        if 'latitude' in X_train_fold.columns and 'longitude' in X_train_fold.columns:\n",
    "            lat_mean = X_train_fold['latitude'].mean()\n",
    "            lon_mean = X_train_fold['longitude'].mean()\n",
    "            \n",
    "            train_coords_temp = X_train_fold[['latitude', 'longitude']].copy()\n",
    "            valid_coords_temp = X_valid_fold[['latitude', 'longitude']].copy()\n",
    "            \n",
    "            train_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "            train_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "            valid_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "            valid_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "            kmeans.fit(train_coords_temp)\n",
    "            \n",
    "            X_train_fold['cluster'] = kmeans.predict(train_coords_temp)\n",
    "            X_valid_fold['cluster'] = kmeans.predict(valid_coords_temp)\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "        )\n",
    "        \n",
    "        y_pred_fold = model.predict(X_valid_fold)\n",
    "        y_pred_orig_scale = np.expm1(y_pred_fold)\n",
    "        y_valid_orig_scale = np.expm1(y_valid_fold)\n",
    "        \n",
    "        exp_score = competition_score(y_valid_orig_scale, y_pred_orig_scale)\n",
    "        fold_scores.append(exp_score)\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(X_train, y_train, params, model_type):\n",
    "    \"\"\"\n",
    "    Trains a model (XGBoost or LightGBM) after performing feature engineering.\n",
    "\n",
    "    This function combines preprocessing steps like cyclical feature creation and\n",
    "    KMeans clustering with model training. It handles parameter cleaning for\n",
    "    each model type and attaches the fitted preprocessors (like KMeans) to \n",
    "    the trained model object for use in prediction pipelines.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The training feature data.\n",
    "        y_train (pd.Series): The training target data.\n",
    "        params (dict): A dictionary of hyperparameters for the model.\n",
    "                       Can include 'n_clusters' for KMeans.\n",
    "        model_type (str): The type of model to train, either 'xgb' or 'lgb'.\n",
    "\n",
    "    Returns:\n",
    "        A trained model object (XGBRegressor or LGBMRegressor) with\n",
    "        preprocessing information (kmeans_, lat_mean_, lon_mean_) attached.\n",
    "    \"\"\"\n",
    "    # 1. Preprocessing\n",
    "    X_train_processed = create_cyclical_features(X_train.copy())\n",
    "    \n",
    "    # Initialize preprocessor attributes to None\n",
    "    kmeans = None\n",
    "    lat_mean = None\n",
    "    lon_mean = None\n",
    "    \n",
    "    # Perform KMeans clustering if location data is available\n",
    "    if 'latitude' in X_train_processed.columns and 'longitude' in X_train_processed.columns:\n",
    "        n_clusters = params.get('n_clusters', 30)\n",
    "        lat_mean = X_train_processed['latitude'].mean()\n",
    "        lon_mean = X_train_processed['longitude'].mean()\n",
    "        \n",
    "        # Create a temporary dataframe for clustering, handling potential NaNs\n",
    "        coords_temp = X_train_processed[['latitude', 'longitude']].copy()\n",
    "        coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "        coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "        kmeans.fit(coords_temp)\n",
    "        X_train_processed['cluster'] = kmeans.predict(coords_temp)\n",
    "    \n",
    "    # 2. Target Transformation\n",
    "    y_train_log = np.log1p(y_train)\n",
    "    \n",
    "    # 3. Model-specific Training\n",
    "    # Remove non-model parameters before passing to the model constructor\n",
    "    model_params = {k: v for k, v in params.items() if k != 'n_clusters'}\n",
    "    \n",
    "    if model_type == 'xgb':\n",
    "        model = xgb.XGBRegressor(**model_params)\n",
    "        model.fit(X_train_processed, y_train_log)\n",
    "    elif model_type == 'lgb':\n",
    "        model = lgb.LGBMRegressor(**model_params)\n",
    "        model.fit(X_train_processed, y_train_log)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: '{model_type}'. Choose 'xgb' or 'lgb'.\")\n",
    "    \n",
    "    # 4. Attach preprocessing information to the trained model\n",
    "    model.kmeans_ = kmeans\n",
    "    model.lat_mean_ = lat_mean\n",
    "    model.lon_mean_ = lon_mean\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_ensemble(X_train, y_train, X_test, xgb_params, lgb_params, tss):\n",
    "    \"\"\"Create ensemble with separately optimized XGBoost and LightGBM\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    X_test_processed = create_cyclical_features(X_test.copy())\n",
    "    \n",
    "    xgb_n_clusters = xgb_params.get('n_clusters', 30)\n",
    "    lgb_n_clusters = lgb_params.get('n_clusters', 30)\n",
    "    \n",
    "    if 'latitude' in X_test_processed.columns and 'longitude' in X_test_processed.columns:\n",
    "        X_train_temp = create_cyclical_features(X_train.copy())\n",
    "        lat_mean = X_train_temp['latitude'].mean()\n",
    "        lon_mean = X_train_temp['longitude'].mean()\n",
    "        \n",
    "        test_coords_temp = X_test_processed[['latitude', 'longitude']].copy()\n",
    "        test_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "        test_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "        \n",
    "        train_coords_temp = X_train_temp[['latitude', 'longitude']].copy()\n",
    "        train_coords_temp['latitude'].fillna(lat_mean, inplace=True)\n",
    "        train_coords_temp['longitude'].fillna(lon_mean, inplace=True)\n",
    "    \n",
    "    # XGBoost with optimal clustering\n",
    "    X_test_xgb = X_test_processed.copy()\n",
    "    if 'latitude' in X_test_xgb.columns:\n",
    "        kmeans_xgb = KMeans(n_clusters=xgb_n_clusters, random_state=42, n_init='auto')\n",
    "        kmeans_xgb.fit(train_coords_temp)\n",
    "        X_test_xgb['cluster'] = kmeans_xgb.predict(test_coords_temp)\n",
    "    \n",
    "    print(\"Training XGBoost with optimized parameters...\")\n",
    "    xgb_model = train_final_model(X_train, y_train, xgb_params, model_type='xgb')\n",
    "    pred_xgb = xgb_model.predict(X_test_xgb)\n",
    "    predictions.append(('xgb_optimized', np.expm1(pred_xgb), 0.45))\n",
    "    \n",
    "    # LightGBM with optimal clustering\n",
    "    X_test_lgb = X_test_processed.copy()\n",
    "    if 'latitude' in X_test_lgb.columns:\n",
    "        kmeans_lgb = KMeans(n_clusters=lgb_n_clusters, random_state=42, n_init='auto')\n",
    "        kmeans_lgb.fit(train_coords_temp)\n",
    "        X_test_lgb['cluster'] = kmeans_lgb.predict(test_coords_temp)\n",
    "    \n",
    "    print(\"Training LightGBM with optimized parameters...\")\n",
    "    lgb_model = train_final_model(X_train, y_train, lgb_params, model_type='lgb')\n",
    "    pred_lgb = lgb_model.predict(X_test_lgb)\n",
    "    predictions.append(('lgb_optimized', np.expm1(pred_lgb), 0.35))\n",
    "    \n",
    "    # XGBoost variant\n",
    "    print(\"Training XGBoost variant...\")\n",
    "    xgb_variant_params = xgb_params.copy()\n",
    "    xgb_variant_params['max_depth'] = min(xgb_params['max_depth'] + 2, 15)\n",
    "    xgb_variant_params['learning_rate'] = xgb_params['learning_rate'] * 0.8\n",
    "    xgb_variant_params['n_estimators'] = int(xgb_params['n_estimators'] * 1.2)\n",
    "    \n",
    "    xgb_variant_model = train_final_model(X_train, y_train, xgb_variant_params, model_type='xgb')\n",
    "    pred_xgb_variant = xgb_variant_model.predict(X_test_xgb)\n",
    "    predictions.append(('xgb_variant', np.expm1(pred_xgb_variant), 0.15))\n",
    "    \n",
    "    # LightGBM variant\n",
    "    print(\"Training LightGBM variant...\")\n",
    "    lgb_variant_params = lgb_params.copy()\n",
    "    lgb_variant_params['num_leaves'] = int(lgb_params.get('num_leaves', 31) * 0.8)\n",
    "    lgb_variant_params['learning_rate'] = lgb_params['learning_rate'] * 0.9\n",
    "    \n",
    "    lgb_variant_model = train_final_model(X_train, y_train, lgb_variant_params, model_type='lgb')\n",
    "    pred_lgb_variant = lgb_variant_model.predict(X_test_lgb)\n",
    "    predictions.append(('lgb_variant', np.expm1(pred_lgb_variant), 0.05))\n",
    "    \n",
    "    # Weighted ensemble\n",
    "    final_pred = np.zeros(len(X_test))\n",
    "    print(\"\\nEnsemble weights:\")\n",
    "    for name, pred, weight in predictions:\n",
    "        print(f\"  {name}: {weight:.2%}\")\n",
    "        final_pred += weight * pred\n",
    "    \n",
    "    return final_pred, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_pipeline(train_df, test_df, target_col='pollution_value', \n",
    "                     n_trials_xgb=100, n_trials_lgb=100, feature_dir=None):\n",
    "    \"\"\"Complete improved pipeline with separate XGBoost and LightGBM optimization\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"IMPROVED PIPELINE - NO LEAKAGE, SEPARATE OPTIMIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X_train = train_df.drop(target_col, axis=1)\n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df.copy()\n",
    "    \n",
    "    test_ids = test_df['id'].values\n",
    "    \n",
    "    if feature_dir and os.path.exists(feature_dir):\n",
    "        generators = load_all_feature_generators(feature_dir)\n",
    "        X_train_generated, X_test_generated = combine_feature_generators(generators, X_train, X_test)\n",
    "    else:\n",
    "        print(\"\\nNo existing feature generators found. Using raw features.\")\n",
    "        X_train_generated = X_train\n",
    "        X_test_generated = X_test\n",
    "    \n",
    "    print(f\"Total features after generation: {X_train_generated.shape[1]}\")\n",
    "    \n",
    "    tss = FixedWindowTimeSeriesSplit(n_splits=5, test_size=2700, gap=0, min_train_size=2700)\n",
    "    \n",
    "    # Optimize XGBoost\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"OPTIMIZING XGBOOST\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    xgb_sampler = optuna.samplers.TPESampler(\n",
    "        multivariate=True, group=True, \n",
    "        n_startup_trials=int(0.1*n_trials_xgb), \n",
    "        constant_liar=True, seed=42\n",
    "    )\n",
    "    xgb_study = optuna.create_study(\n",
    "        direction=\"maximize\", \n",
    "        study_name=\"xgboost_optimization_improved\",\n",
    "        sampler=xgb_sampler,\n",
    "        storage=\"sqlite:///xgb_optuna_improved.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    xgb_objective_func = lambda trial: xgb_objective(\n",
    "        trial, X_train_generated, y_train, tss, n_clusters_range=(20, 50)\n",
    "    )\n",
    "    xgb_study.optimize(xgb_objective_func, n_trials=n_trials_xgb)\n",
    "    \n",
    "    xgb_best_params = xgb_study.best_params.copy()\n",
    "    print(f\"XGBoost best score: {xgb_study.best_value:.4f}\")\n",
    "    print(f\"XGBoost best params: {xgb_best_params}\")\n",
    "    \n",
    "    # Optimize LightGBM\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"OPTIMIZING LIGHTGBM\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    lgb_sampler = optuna.samplers.TPESampler(\n",
    "        multivariate=True, group=True,\n",
    "        n_startup_trials=int(0.1*n_trials_lgb),\n",
    "        constant_liar=True, seed=42\n",
    "    )\n",
    "    lgb_study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"lightgbm_optimization\",\n",
    "        sampler=lgb_sampler,\n",
    "        storage=\"sqlite:///lgb_optuna.db\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    lgb_objective_func = lambda trial: lgb_objective(\n",
    "        trial, X_train_generated, y_train, tss, n_clusters_range=(20, 50)\n",
    "    )\n",
    "    lgb_study.optimize(lgb_objective_func, n_trials=n_trials_lgb)\n",
    "    \n",
    "    lgb_best_params = lgb_study.best_params.copy()\n",
    "    print(f\"LightGBM best score: {lgb_study.best_value:.4f}\")\n",
    "    print(f\"LightGBM best params: {lgb_best_params}\")\n",
    "    \n",
    "    # Generate ensemble predictions\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"GENERATING ENSEMBLE PREDICTIONS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    final_predictions, model_predictions = create_optimized_ensemble(\n",
    "        X_train_generated, y_train, X_test_generated,\n",
    "        xgb_best_params, lgb_best_params, tss\n",
    "    )\n",
    "    \n",
    "    # Simple post-processing: ensure non-negative\n",
    "    final_predictions = np.maximum(final_predictions, 0)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'pollution_value': final_predictions\n",
    "    })\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Mean prediction: {final_predictions.mean():.2f}\")\n",
    "    print(f\"Std prediction: {final_predictions.std():.2f}\")\n",
    "    print(f\"Min prediction: {final_predictions.min():.2f}\")\n",
    "    print(f\"Max prediction: {final_predictions.max():.2f}\")\n",
    "    \n",
    "    print(f\"\\nTraining target stats:\")\n",
    "    print(f\"Mean: {y_train.mean():.2f}\")\n",
    "    print(f\"Std: {y_train.std():.2f}\")\n",
    "    print(f\"Min: {y_train.min():.2f}\")\n",
    "    print(f\"Max: {y_train.max():.2f}\")\n",
    "    \n",
    "    return submission, xgb_study, lgb_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "year = 2023\n",
    "train_df['datetime'] = pd.to_datetime(train_df['day_of_year'], format='%j', errors='coerce') \\\n",
    "                       + pd.to_timedelta(train_df['hour'], unit='h')\n",
    "train_df['datetime'] = train_df['datetime'].apply(\n",
    "    lambda dt: dt.replace(year=year) if pd.notnull(dt) else dt\n",
    ")\n",
    "train_df = train_df.sort_values(by='datetime')\n",
    "train_df = train_df.drop(columns='datetime')\n",
    "train_df.reset_index(drop=True, inplace=True) # CRUCIAL #\n",
    "\n",
    "X_train = train_df.drop(['pollution_value', 'id'], axis=1)\n",
    "y_train = train_df['pollution_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running feature generation run 8...\n",
      "Starting regression on cpu - 7649 samples, 6 features\n",
      "Params: gen=100, parents=40, children=200, limit=100\n",
      "Gen 0: Train rmse_exp=0.91326, Val rmse_exp=0.64187\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005622148513793945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generations",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06360b25c0c4a0392ea25ce652666e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035381317138671875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State saved to model_rredone_10_folds/feature_generator_run_8.pkl\n",
      "Gen 1: Added 7 features, 13 total (7 new). Train rmse_exp=0.92998, Val rmse_exp=0.65622. Score improved by 0.01435. Status: NONE, Strategy success: HM:0.50, BS:0.50, N:1.00\n",
      "  Simple: ['longitude_mean_month', 'longitude_square', 'day_of_week_geometric_mean_month', 'hour_angle_between_month', 'longitude_add_day_of_week', 'day_of_year_pow_day_of_week', 'latitude_percent_change_longitude']\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0036725997924804688,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 2: Added 0 features, 13 total (7 new). Train rmse_exp=0.92998, Val rmse_exp=0.65622. No improvement. Status: NONE, Strategy success: HM:0.50, BS:0.50, N:0.50\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004117250442504883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 3: Added 0 features, 13 total (7 new). Train rmse_exp=0.92998, Val rmse_exp=0.65622. No improvement. Status: MILD, Strategy success: HM:0.50, BS:0.50, N:0.33\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003452301025390625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State saved to model_rredone_10_folds/feature_generator_run_8.pkl\n",
      "Gen 4: Added 1 features, 14 total (8 new). Train rmse_exp=0.92843, Val rmse_exp=0.65790. Score improved by 0.00169. Status: MODERATE, Strategy success: HM:0.50, BS:0.50, N:0.50\n",
      "  Simple: ['longitude_mean_month_arccos']\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0036978721618652344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 5: Added 0 features, 14 total (8 new). Train rmse_exp=0.92843, Val rmse_exp=0.65790. No improvement. Status: MODERATE, Strategy success: HM:0.50, BS:0.50, N:0.40\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0055124759674072266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 6: Added 0 features, 14 total (8 new). Train rmse_exp=0.92843, Val rmse_exp=0.65790. No improvement. Status: MODERATE, Strategy success: HM:0.50, BS:0.50, N:0.33\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003504514694213867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State saved to model_rredone_10_folds/feature_generator_run_8.pkl\n",
      "Gen 7: Added 1 features, 15 total (9 new). Train rmse_exp=0.92936, Val rmse_exp=0.65834. Score improved by 0.00044. Status: MODERATE, Strategy success: HM:0.50, BS:0.50, N:0.43\n",
      "  Simple: ['longitude_sign']\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0037980079650878906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 8: Added 0 features, 15 total (9 new). Train rmse_exp=0.92936, Val rmse_exp=0.65834. No improvement. Status: MODERATE, Strategy success: HM:0.50, BS:0.50, N:0.38\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004578590393066406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 9: Added 0 features, 15 total (9 new). Train rmse_exp=0.92936, Val rmse_exp=0.65834. No improvement. Status: MODERATE, Strategy success: HM:0.50, BS:0.50, N:0.33\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003882169723510742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 10: Added 0 features, 15 total (9 new). Train rmse_exp=0.92936, Val rmse_exp=0.65834. No improvement. Status: MODERATE, Strategy success: HM:0.50, BS:0.50, N:0.30\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004767894744873047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Evaluating features",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd2e041585448849ddcdd456f011569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating features:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stage 1: Run feature generation multiple times\n",
    "for run_id in range(8, 9):\n",
    "    X_generated, generator = run_single_feature_generation(\n",
    "        X_train, y_train, \n",
    "        save_dir=\"model_rredone_10_folds\", \n",
    "        run_id=run_id,\n",
    "        max_new_feats=100,\n",
    "        n_generations=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stage 2: Run complete pipeline\n",
    "submission, xgb_study, lgb_study = enhanced_pipeline(\n",
    "    train_df, \n",
    "    test_df,\n",
    "    target_col='pollution_value',\n",
    "    n_trials_xgb=100,\n",
    "    n_trials_lgb=100,\n",
    "    feature_dir=\"model_rredone\",\n",
    ")\n",
    "\n",
    "submission.to_csv('submission_enhanced.csv', index=False)\n",
    "print(\"Submission saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
